# FlakeGuard Prometheus Alerting Rules
# 
# Multi-window burn-rate alerts following Google SRE best practices
# Implements 1h & 6h burn-rate thresholds for critical SLOs

groups:
  # ============================================================================
  # Multi-Window Burn-Rate Alerts for Ingestion Error Ratio
  # ============================================================================
  
  - name: flakeguard.ingestion.slo
    interval: 30s
    rules:
      # Fast burn: Alert if we're burning through error budget too quickly
      # 1h window: 72x burn rate (exhausts monthly budget in 10 hours)  
      - alert: IngestionErrorRateFastBurn
        expr: |
          (
            (1 - rate(flakeguard_api_parse_results_total{result="success"}[1h]) / rate(flakeguard_api_parse_results_total[1h])) > (72 * 0.01)
            and
            (1 - rate(flakeguard_api_parse_results_total{result="success"}[5m]) / rate(flakeguard_api_parse_results_total[5m])) > (72 * 0.01)
          )
        for: 2m
        labels:
          severity: page
          service: flakeguard
          component: ingestion
          slo: parse_success_rate
          burn_rate: fast
        annotations:
          summary: "FlakeGuard ingestion error rate is burning error budget too fast"
          description: |
            The ingestion parse error rate is {{ $value | humanizePercentage }} over the last hour,
            which is 72x the allowed burn rate. At this rate, we'll exhaust our monthly error budget in ~10 hours.
            
            Repository: {{ $labels.repository }}
            Current error rate: {{ $value | humanizePercentage }}
            SLO target: 99% success rate (1% error budget)
            
            This requires immediate attention to prevent SLO breach.
          runbook_url: "https://runbooks.flakeguard.dev/ingestion-error-rate"
          
      # Slow burn: Alert if error rate is elevated over longer period
      # 6h window: 6x burn rate (exhausts monthly budget in 5 days)
      - alert: IngestionErrorRateSlowBurn  
        expr: |
          (
            (1 - rate(flakeguard_api_parse_results_total{result="success"}[6h]) / rate(flakeguard_api_parse_results_total[6h])) > (6 * 0.01)
            and
            (1 - rate(flakeguard_api_parse_results_total{result="success"}[30m]) / rate(flakeguard_api_parse_results_total[30m])) > (6 * 0.01)
          )
        for: 15m
        labels:
          severity: warning
          service: flakeguard
          component: ingestion
          slo: parse_success_rate
          burn_rate: slow
        annotations:
          summary: "FlakeGuard ingestion error rate elevated over 6h window"
          description: |
            The ingestion parse error rate is {{ $value | humanizePercentage }} over the last 6 hours,
            which is 6x the allowed burn rate. At this rate, we'll exhaust our monthly error budget in ~5 days.
            
            Repository: {{ $labels.repository }}
            Current error rate: {{ $value | humanizePercentage }}
            SLO target: 99% success rate (1% error budget)
            
            Investigation and remediation should be prioritized.
          runbook_url: "https://runbooks.flakeguard.dev/ingestion-error-rate"

  # ============================================================================
  # Multi-Window Burn-Rate Alerts for API 5xx Rate  
  # ============================================================================
  
  - name: flakeguard.api.slo
    interval: 30s
    rules:
      # Fast burn: API 5xx errors burning error budget rapidly
      # 1h window: 1440x burn rate (exhausts monthly budget in 30 minutes)
      - alert: API5xxRateFastBurn
        expr: |
          (
            rate(flakeguard_api_http_requests_total{status_code=~"5.."}[1h]) / rate(flakeguard_api_http_requests_total[1h]) > (1440 * 0.001)
            and  
            rate(flakeguard_api_http_requests_total{status_code=~"5.."}[5m]) / rate(flakeguard_api_http_requests_total[5m]) > (1440 * 0.001)
          )
        for: 2m
        labels:
          severity: page
          service: flakeguard
          component: api
          slo: availability
          burn_rate: fast
        annotations:
          summary: "FlakeGuard API 5xx error rate is critically high"
          description: |
            API 5xx error rate is {{ $value | humanizePercentage }} over the last hour,
            which is 1440x the allowed burn rate. At this rate, we'll exhaust our monthly error budget in ~30 minutes.
            
            Route: {{ $labels.route }}
            Method: {{ $labels.method }}
            Current 5xx rate: {{ $value | humanizePercentage }}
            SLO target: 99.9% availability (0.1% error budget)
            
            IMMEDIATE ACTION REQUIRED - Service availability at risk!
          runbook_url: "https://runbooks.flakeguard.dev/api-5xx-errors"
          
      # Slow burn: Elevated 5xx rate over longer period  
      # 6h window: 6x burn rate (exhausts monthly budget in 5 days)
      - alert: API5xxRateSlowBurn
        expr: |
          (
            rate(flakeguard_api_http_requests_total{status_code=~"5.."}[6h]) / rate(flakeguard_api_http_requests_total[6h]) > (6 * 0.001)
            and
            rate(flakeguard_api_http_requests_total{status_code=~"5.."}[30m]) / rate(flakeguard_api_http_requests_total[30m]) > (6 * 0.001)
          )
        for: 15m
        labels:
          severity: warning  
          service: flakeguard
          component: api
          slo: availability
          burn_rate: slow
        annotations:
          summary: "FlakeGuard API 5xx error rate elevated"
          description: |
            API 5xx error rate is {{ $value | humanizePercentage }} over the last 6 hours,
            which is 6x the allowed burn rate. At this rate, we'll exhaust our monthly error budget in ~5 days.
            
            Route: {{ $labels.route }}
            Method: {{ $labels.method }}  
            Current 5xx rate: {{ $value | humanizePercentage }}
            SLO target: 99.9% availability (0.1% error budget)
            
            Trend monitoring and investigation recommended.
          runbook_url: "https://runbooks.flakeguard.dev/api-5xx-errors"

  # ============================================================================
  # Additional SLO-based Alerts
  # ============================================================================
  
  - name: flakeguard.latency.slo
    interval: 30s  
    rules:
      # Ingestion latency SLO breach
      - alert: IngestionLatencySLOBreach
        expr: |
          histogram_quantile(0.95, rate(flakeguard_api_ingestion_latency_seconds_bucket[5m])) > 30
        for: 10m
        labels:
          severity: warning
          service: flakeguard
          component: ingestion
          slo: latency
        annotations:
          summary: "FlakeGuard ingestion P95 latency exceeds SLO"
          description: |
            Ingestion P95 latency is {{ $value }}s, exceeding the SLO target of 30s.
            
            Repository: {{ $labels.repository }}
            Content Type: {{ $labels.content_type }}
            
            This may impact user experience and indicates performance degradation.
          runbook_url: "https://runbooks.flakeguard.dev/ingestion-latency"
          
      # Check run delivery latency SLO breach
      - alert: CheckRunDeliveryLatencySLOBreach
        expr: |
          histogram_quantile(0.95, rate(flakeguard_api_check_run_delivery_seconds_bucket[5m])) > 60
        for: 10m  
        labels:
          severity: warning
          service: flakeguard
          component: github-integration
          slo: latency
        annotations:
          summary: "FlakeGuard check run delivery P95 latency exceeds SLO"
          description: |
            Check run delivery P95 latency is {{ $value }}s, exceeding the SLO target of 60s.
            
            Repository: {{ $labels.repository }}
            
            This may delay feedback to developers and impact the user experience.
          runbook_url: "https://runbooks.flakeguard.dev/check-run-delivery"

  # ============================================================================
  # Operational Health Alerts
  # ============================================================================
  
  - name: flakeguard.operational
    interval: 30s
    rules:
      # High memory usage
      - alert: FlakeGuardHighMemoryUsage
        expr: |
          flakeguard_api_memory_usage_bytes{type="rss"} / 1024 / 1024 > 512
        for: 5m
        labels:
          severity: warning
          service: flakeguard  
          component: "{{ $labels.job }}"
        annotations:
          summary: "FlakeGuard service using high memory"
          description: |
            FlakeGuard {{ $labels.job }} is using {{ $value }}MB of memory, exceeding 512MB threshold.
            
            This may indicate a memory leak or increased load.
          runbook_url: "https://runbooks.flakeguard.dev/high-memory-usage"
          
      # Database connection issues
      - alert: FlakeGuardDatabaseConnectionDown
        expr: |
          flakeguard_api_database_connections_active == 0
        for: 1m
        labels:
          severity: page
          service: flakeguard
          component: database
        annotations:
          summary: "FlakeGuard database connection down"
          description: |
            FlakeGuard has no active database connections.
            
            This will cause service degradation and requires immediate attention.
          runbook_url: "https://runbooks.flakeguard.dev/database-connection-issues"
          
      # Worker queue backing up  
      - alert: FlakeGuardWorkerQueueBacklog
        expr: |
          flakeguard_worker_queue_size{status="waiting"} > 100
        for: 5m
        labels:
          severity: warning
          service: flakeguard
          component: worker
        annotations:
          summary: "FlakeGuard worker queue has significant backlog"
          description: |
            Worker queue {{ $labels.queue }} has {{ $value }} waiting jobs, exceeding normal capacity.
            
            This may indicate worker performance issues or increased load.
          runbook_url: "https://runbooks.flakeguard.dev/worker-queue-backlog"

  # ============================================================================
  # Business Logic Alerts  
  # ============================================================================
  
  - name: flakeguard.business
    interval: 60s
    rules:
      # High flake detection rate (could indicate test infrastructure issues)
      - alert: FlakeGuardHighFlakeDetectionRate
        expr: |
          rate(flakeguard_api_flake_detections_total[1h]) > 10
        for: 30m
        labels:
          severity: info
          service: flakeguard
          component: analysis
        annotations:
          summary: "FlakeGuard detecting unusually high flake rate"
          description: |
            FlakeGuard is detecting {{ $value }} flaky tests per hour in repository {{ $labels.repository }}.
            
            This may indicate:
            - Test infrastructure issues
            - Environmental problems
            - Need for quarantine policy review
            
            Consider investigating test environment stability.
          runbook_url: "https://runbooks.flakeguard.dev/high-flake-rate"
          
      # No recent test activity (could indicate integration issues)
      - alert: FlakeGuardNoRecentTestActivity  
        expr: |
          flakeguard_api_active_repositories == 0
        for: 24h
        labels:
          severity: warning
          service: flakeguard
          component: ingestion
        annotations:
          summary: "FlakeGuard has no active repositories in 24h"
          description: |
            FlakeGuard has not processed any test results for 24 hours.
            
            This may indicate:
            - GitHub webhook delivery issues
            - Integration configuration problems  
            - Repository inactivity
            
            Verify webhook configuration and repository activity.
          runbook_url: "https://runbooks.flakeguard.dev/no-test-activity"