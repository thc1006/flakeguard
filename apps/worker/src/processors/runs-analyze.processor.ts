/**
 * Workflow Runs Analysis Processor
 * 
 * Analyzes test results to calculate flakiness scores, detect patterns,
 * and update GitHub Check Runs with flakiness information and recommendations.
 */

import { Job } from 'bullmq';
import { PrismaClient } from '@prisma/client';
import { Octokit } from '@octokit/rest';
import { logger } from '../utils/logger.js';
import { 
  recordJobCompletion, 
  recordFlakinessAnalysis,\n  recordGitHubApiCall \n} from '../utils/metrics.js';\nimport { \n  FLAKINESS_CONFIG,\n  QueueNames \n} from '@flakeguard/shared';\n\n// ============================================================================\n// Types and Interfaces\n// ============================================================================\n\nexport interface RunsAnalyzeJobData {\n  workflowRunId: number;\n  repository: {\n    owner: string;\n    repo: string;\n    installationId: number;\n  };\n  correlationId?: string;\n  priority: 'low' | 'normal' | 'high' | 'critical';\n  forceRecompute?: boolean;\n  analysisConfig?: {\n    lookbackDays?: number;\n    minRunsThreshold?: number;\n    flakinessThreshold?: number;\n  };\n}\n\nexport interface AnalysisResult {\n  success: boolean;\n  workflowRunId: number;\n  analyzedTests: number;\n  flakyTests: FlakyTestResult[];\n  overallFlakinessScore: number;\n  recommendations: string[];\n  checkRunUpdated: boolean;\n  processingTimeMs: number;\n  errors: string[];\n  warnings: string[];\n}\n\nexport interface FlakyTestResult {\n  testName: string;\n  className: string;\n  flakinessScore: number;\n  totalRuns: number;\n  failures: number;\n  successRate: number;\n  recentFailures: number;\n  pattern: 'intermittent' | 'environmental' | 'timing' | 'unknown';\n  severity: 'low' | 'medium' | 'high' | 'critical';\n  recommendation: string;\n  firstSeen: string;\n  lastSeen: string;\n  affectedBranches: string[];\n}\n\nexport interface TestExecutionHistory {\n  testName: string;\n  className: string;\n  executions: TestExecution[];\n}\n\nexport interface TestExecution {\n  workflowRunId: number;\n  status: 'passed' | 'failed' | 'error' | 'skipped';\n  executionTime: number;\n  timestamp: Date;\n  branch: string;\n  commitSha: string;\n  runNumber: number;\n  failureMessage?: string;\n  errorMessage?: string;\n}\n\nexport interface FlakinessMetrics {\n  totalTests: number;\n  flakyTests: number;\n  overallFlakinessRate: number;\n  averageFlakinessScore: number;\n  severelyFlakyTests: number;\n  newlyFlakyTests: number;\n  improvedTests: number;\n  regressionTests: number;\n}\n\n// ============================================================================\n// Processor Implementation\n// ============================================================================\n\n/**\n * Create runs analysis processor\n */\nexport function createRunsAnalyzeProcessor(\n  prisma: PrismaClient,\n  octokit?: Octokit\n) {\n  return async function processRunsAnalyze(\n    job: Job<RunsAnalyzeJobData>\n  ): Promise<AnalysisResult> {\n    const { data } = job;\n    const startTime = Date.now();\n    \n    logger.info({\n      jobId: job.id,\n      workflowRunId: data.workflowRunId,\n      repository: `${data.repository.owner}/${data.repository.repo}`,\n      correlationId: data.correlationId,\n      priority: data.priority,\n      forceRecompute: data.forceRecompute\n    }, 'Processing runs analysis job');\n\n    try {\n      // Update job progress\n      await job.updateProgress({\n        phase: 'analyzing',\n        percentage: 10,\n        message: 'Loading test history'\n      });\n\n      // Load test execution history\n      const testHistories = await loadTestExecutionHistories(prisma, data);\n      \n      if (testHistories.length === 0) {\n        logger.info({ workflowRunId: data.workflowRunId }, 'No test history found for analysis');\n        return createEmptyAnalysisResult(data.workflowRunId, startTime);\n      }\n\n      // Update progress\n      await job.updateProgress({\n        phase: 'calculating',\n        percentage: 30,\n        message: `Analyzing ${testHistories.length} test patterns`\n      });\n\n      // Calculate flakiness scores\n      const flakyTests = await calculateFlakinessScores(testHistories, data.analysisConfig);\n      \n      // Update progress\n      await job.updateProgress({\n        phase: 'storing',\n        percentage: 60,\n        message: 'Storing analysis results'\n      });\n\n      // Store analysis results\n      await storeAnalysisResults(prisma, data, flakyTests);\n      \n      // Update progress\n      await job.updateProgress({\n        phase: 'updating',\n        percentage: 80,\n        message: 'Updating GitHub Check Run'\n      });\n\n      // Update GitHub Check Run with results\n      const github = octokit || createMockGitHubClient();\n      const checkRunUpdated = await updateGitHubCheckRun(github, data, flakyTests);\n      \n      // Generate recommendations\n      const recommendations = generateRecommendations(flakyTests);\n      \n      // Calculate overall metrics\n      const overallFlakinessScore = calculateOverallFlakinessScore(flakyTests);\n      \n      const processingTimeMs = Date.now() - startTime;\n      \n      // Final progress update\n      await job.updateProgress({\n        phase: 'complete',\n        percentage: 100,\n        message: 'Analysis complete'\n      });\n\n      const result: AnalysisResult = {\n        success: true,\n        workflowRunId: data.workflowRunId,\n        analyzedTests: testHistories.length,\n        flakyTests,\n        overallFlakinessScore,\n        recommendations,\n        checkRunUpdated,\n        processingTimeMs,\n        errors: [],\n        warnings: []\n      };\n\n      // Record metrics\n      recordFlakinessAnalysis(\n        `${data.repository.owner}/${data.repository.repo}`,\n        testHistories.length,\n        flakyTests.length,\n        overallFlakinessScore,\n        processingTimeMs\n      );\n      \n      recordJobCompletion(QueueNames.RUNS_ANALYZE, 'completed', data.priority, processingTimeMs);\n      \n      logger.info({\n        jobId: job.id,\n        workflowRunId: data.workflowRunId,\n        analyzedTests: testHistories.length,\n        flakyTests: flakyTests.length,\n        overallFlakinessScore,\n        processingTimeMs\n      }, 'Runs analysis completed successfully');\n\n      return result;\n\n    } catch (error) {\n      const processingTimeMs = Date.now() - startTime;\n      const errorMessage = error instanceof Error ? error.message : String(error);\n      \n      recordJobCompletion(QueueNames.RUNS_ANALYZE, 'failed', data.priority, processingTimeMs, 'analysis_error');\n      \n      logger.error({\n        jobId: job.id,\n        workflowRunId: data.workflowRunId,\n        error: errorMessage,\n        stack: error instanceof Error ? error.stack : undefined,\n        processingTimeMs\n      }, 'Runs analysis failed');\n\n      throw error;\n    }\n  };\n}\n\n// ============================================================================\n// Data Loading Functions\n// ============================================================================\n\n/**\n * Load test execution histories for analysis\n */\nasync function loadTestExecutionHistories(\n  prisma: PrismaClient,\n  data: RunsAnalyzeJobData\n): Promise<TestExecutionHistory[]> {\n  const lookbackDays = data.analysisConfig?.lookbackDays || FLAKINESS_CONFIG.ANALYSIS_WINDOW_DAYS;\n  const minRuns = data.analysisConfig?.minRunsThreshold || FLAKINESS_CONFIG.MIN_RUNS_FOR_ANALYSIS;\n  \n  const cutoffDate = new Date();\n  cutoffDate.setDate(cutoffDate.getDate() - lookbackDays);\n  \n  try {\n    // Query test cases with their execution history\n    const testCasesWithHistory = await prisma.testCase.findMany({\n      where: {\n        testSuite: {\n          workflowRun: {\n            repositoryOwner: data.repository.owner,\n            repositoryName: data.repository.repo,\n            createdAt: {\n              gte: cutoffDate\n            }\n          }\n        }\n      },\n      include: {\n        testSuite: {\n          include: {\n            workflowRun: true\n          }\n        }\n      },\n      orderBy: [\n        { className: 'asc' },\n        { name: 'asc' },\n        { createdAt: 'desc' }\n      ]\n    });\n    \n    // Group by test name and class name\n    const testHistoryMap = new Map<string, TestExecutionHistory>();\n    \n    for (const testCase of testCasesWithHistory) {\n      const testKey = `${testCase.className}#${testCase.name}`;\n      \n      if (!testHistoryMap.has(testKey)) {\n        testHistoryMap.set(testKey, {\n          testName: testCase.name,\n          className: testCase.className,\n          executions: []\n        });\n      }\n      \n      const history = testHistoryMap.get(testKey)!;\n      history.executions.push({\n        workflowRunId: testCase.testSuite.workflowRunId,\n        status: testCase.status,\n        executionTime: testCase.time,\n        timestamp: testCase.createdAt,\n        branch: testCase.testSuite.workflowRun.headBranch,\n        commitSha: testCase.testSuite.workflowRun.headSha,\n        runNumber: testCase.testSuite.workflowRun.runNumber,\n        failureMessage: testCase.failureMessage || undefined,\n        errorMessage: testCase.errorMessage || undefined\n      });\n    }\n    \n    // Filter tests with sufficient execution history\n    const result = Array.from(testHistoryMap.values())\n      .filter(history => history.executions.length >= minRuns);\n    \n    logger.debug({\n      totalTestCases: testCasesWithHistory.length,\n      uniqueTests: testHistoryMap.size,\n      testsWithSufficientHistory: result.length,\n      minRuns,\n      lookbackDays\n    }, 'Loaded test execution histories');\n    \n    return result;\n    \n  } catch (error) {\n    logger.error({\n      repository: `${data.repository.owner}/${data.repository.repo}`,\n      error: error instanceof Error ? error.message : String(error)\n    }, 'Failed to load test execution histories');\n    throw error;\n  }\n}\n\n// ============================================================================\n// Flakiness Analysis Functions\n// ============================================================================\n\n/**\n * Calculate flakiness scores for test histories\n */\nasync function calculateFlakinessScores(\n  testHistories: TestExecutionHistory[],\n  config?: RunsAnalyzeJobData['analysisConfig']\n): Promise<FlakyTestResult[]> {\n  const flakinessThreshold = config?.flakinessThreshold || FLAKINESS_CONFIG.FLAKINESS_THRESHOLD;\n  const flakyTests: FlakyTestResult[] = [];\n  \n  for (const history of testHistories) {\n    const analysis = analyzeTestFlakiness(history);\n    \n    if (analysis.flakinessScore >= flakinessThreshold) {\n      flakyTests.push(analysis);\n    }\n  }\n  \n  // Sort by flakiness score (highest first)\n  flakyTests.sort((a, b) => b.flakinessScore - a.flakinessScore);\n  \n  return flakyTests;\n}\n\n/**\n * Analyze individual test flakiness\n */\nfunction analyzeTestFlakiness(history: TestExecutionHistory): FlakyTestResult {\n  const executions = history.executions;\n  const totalRuns = executions.length;\n  \n  // Count outcomes\n  const failures = executions.filter(e => e.status === 'failed' || e.status === 'error').length;\n  const successes = executions.filter(e => e.status === 'passed').length;\n  const skipped = executions.filter(e => e.status === 'skipped').length;\n  \n  // Calculate basic metrics\n  const successRate = successes / (totalRuns - skipped);\n  const failureRate = failures / (totalRuns - skipped);\n  \n  // Calculate flakiness score using multiple factors\n  let flakinessScore = 0;\n  \n  // Factor 1: Failure rate (0-1)\n  flakinessScore += failureRate * 0.4;\n  \n  // Factor 2: Inconsistency penalty\n  const inconsistencyPenalty = calculateInconsistencyPenalty(executions);\n  flakinessScore += inconsistencyPenalty * 0.3;\n  \n  // Factor 3: Recency factor (recent failures are more important)\n  const recencyFactor = calculateRecencyFactor(executions);\n  flakinessScore += recencyFactor * 0.2;\n  \n  // Factor 4: Branch diversity (flaky across multiple branches)\n  const branchDiversityFactor = calculateBranchDiversityFactor(executions);\n  flakinessScore += branchDiversityFactor * 0.1;\n  \n  // Normalize to 0-1 range\n  flakinessScore = Math.min(flakinessScore, 1.0);\n  \n  // Count recent failures (last 10 runs)\n  const recentExecutions = executions.slice(0, Math.min(10, executions.length));\n  const recentFailures = recentExecutions.filter(e => e.status === 'failed' || e.status === 'error').length;\n  \n  // Detect flakiness pattern\n  const pattern = detectFlakinessPattern(executions);\n  \n  // Determine severity\n  const severity = determineSeverity(flakinessScore, recentFailures, totalRuns);\n  \n  // Generate recommendation\n  const recommendation = generateTestRecommendation(flakinessScore, pattern, recentFailures);\n  \n  // Get affected branches\n  const affectedBranches = Array.from(new Set(\n    executions\n      .filter(e => e.status === 'failed' || e.status === 'error')\n      .map(e => e.branch)\n  ));\n  \n  return {\n    testName: history.testName,\n    className: history.className,\n    flakinessScore,\n    totalRuns,\n    failures,\n    successRate,\n    recentFailures,\n    pattern,\n    severity,\n    recommendation,\n    firstSeen: executions[executions.length - 1]?.timestamp.toISOString() || new Date().toISOString(),\n    lastSeen: executions[0]?.timestamp.toISOString() || new Date().toISOString(),\n    affectedBranches\n  };\n}\n\n/**\n * Calculate inconsistency penalty (alternating pass/fail patterns)\n */\nfunction calculateInconsistencyPenalty(executions: TestExecution[]): number {\n  if (executions.length < 3) return 0;\n  \n  let transitions = 0;\n  let previousStatus = executions[0].status;\n  \n  for (let i = 1; i < executions.length; i++) {\n    const currentStatus = executions[i].status;\n    if ((previousStatus === 'passed' && (currentStatus === 'failed' || currentStatus === 'error')) ||\n        ((previousStatus === 'failed' || previousStatus === 'error') && currentStatus === 'passed')) {\n      transitions++;\n    }\n    previousStatus = currentStatus;\n  }\n  \n  // Normalize by execution count\n  return Math.min(transitions / (executions.length - 1), 1.0);\n}\n\n/**\n * Calculate recency factor (weight recent failures more heavily)\n */\nfunction calculateRecencyFactor(executions: TestExecution[]): number {\n  const recentWindow = Math.min(10, executions.length);\n  const recentExecutions = executions.slice(0, recentWindow);\n  \n  let weightedFailures = 0;\n  let totalWeight = 0;\n  \n  for (let i = 0; i < recentExecutions.length; i++) {\n    const weight = (recentWindow - i) / recentWindow; // Higher weight for more recent\n    totalWeight += weight;\n    \n    if (recentExecutions[i].status === 'failed' || recentExecutions[i].status === 'error') {\n      weightedFailures += weight;\n    }\n  }\n  \n  return totalWeight > 0 ? weightedFailures / totalWeight : 0;\n}\n\n/**\n * Calculate branch diversity factor\n */\nfunction calculateBranchDiversityFactor(executions: TestExecution[]): number {\n  const branchFailures = new Map<string, number>();\n  const branchCounts = new Map<string, number>();\n  \n  for (const execution of executions) {\n    const branch = execution.branch;\n    branchCounts.set(branch, (branchCounts.get(branch) || 0) + 1);\n    \n    if (execution.status === 'failed' || execution.status === 'error') {\n      branchFailures.set(branch, (branchFailures.get(branch) || 0) + 1);\n    }\n  }\n  \n  // Count branches with failures\n  const branchesWithFailures = branchFailures.size;\n  const totalBranches = branchCounts.size;\n  \n  return totalBranches > 1 ? branchesWithFailures / totalBranches : 0;\n}\n\n/**\n * Detect flakiness pattern\n */\nfunction detectFlakinessPattern(executions: TestExecution[]): 'intermittent' | 'environmental' | 'timing' | 'unknown' {\n  // Analyze failure messages for common patterns\n  const failureMessages = executions\n    .filter(e => e.failureMessage || e.errorMessage)\n    .map(e => (e.failureMessage || e.errorMessage || '').toLowerCase());\n  \n  if (failureMessages.length === 0) {\n    return 'unknown';\n  }\n  \n  // Check for timing-related issues\n  const timingKeywords = ['timeout', 'wait', 'async', 'race', 'timing', 'delay'];\n  if (failureMessages.some(msg => timingKeywords.some(keyword => msg.includes(keyword)))) {\n    return 'timing';\n  }\n  \n  // Check for environmental issues\n  const envKeywords = ['connection', 'network', 'unavailable', 'service', 'port', 'bind'];\n  if (failureMessages.some(msg => envKeywords.some(keyword => msg.includes(keyword)))) {\n    return 'environmental';\n  }\n  \n  // Default to intermittent\n  return 'intermittent';\n}\n\n/**\n * Determine severity level\n */\nfunction determineSeverity(\n  flakinessScore: number,\n  recentFailures: number,\n  totalRuns: number\n): 'low' | 'medium' | 'high' | 'critical' {\n  if (flakinessScore >= 0.7 || recentFailures >= 5) {\n    return 'critical';\n  } else if (flakinessScore >= 0.5 || recentFailures >= 3) {\n    return 'high';\n  } else if (flakinessScore >= 0.3 || recentFailures >= 2) {\n    return 'medium';\n  } else {\n    return 'low';\n  }\n}\n\n/**\n * Generate test-specific recommendation\n */\nfunction generateTestRecommendation(\n  flakinessScore: number,\n  pattern: string,\n  recentFailures: number\n): string {\n  if (flakinessScore >= 0.7) {\n    return 'Consider quarantining this test and investigating root cause immediately.';\n  } else if (pattern === 'timing') {\n    return 'Add explicit waits, increase timeouts, or use more reliable synchronization mechanisms.';\n  } else if (pattern === 'environmental') {\n    return 'Review external dependencies, network configurations, and service availability.';\n  } else if (recentFailures >= 3) {\n    return 'Monitor closely and consider temporarily skipping until issue is resolved.';\n  } else {\n    return 'Monitor for patterns and consider adding more robust assertions.';\n  }\n}\n\n/**\n * Calculate overall flakiness score\n */\nfunction calculateOverallFlakinessScore(flakyTests: FlakyTestResult[]): number {\n  if (flakyTests.length === 0) return 0;\n  \n  const totalScore = flakyTests.reduce((sum, test) => sum + test.flakinessScore, 0);\n  return totalScore / flakyTests.length;\n}\n\n// ============================================================================\n// Database Storage Functions\n// ============================================================================\n\n/**\n * Store analysis results in database\n */\nasync function storeAnalysisResults(\n  prisma: PrismaClient,\n  data: RunsAnalyzeJobData,\n  flakyTests: FlakyTestResult[]\n): Promise<void> {\n  try {\n    await prisma.$transaction(async (tx) => {\n      // Store or update flaky test records\n      for (const flakyTest of flakyTests) {\n        await tx.flakyTest.upsert({\n          where: {\n            repositoryOwner_repositoryName_className_testName: {\n              repositoryOwner: data.repository.owner,\n              repositoryName: data.repository.repo,\n              className: flakyTest.className,\n              testName: flakyTest.testName\n            }\n          },\n          update: {\n            flakinessScore: flakyTest.flakinessScore,\n            totalRuns: flakyTest.totalRuns,\n            failures: flakyTest.failures,\n            successRate: flakyTest.successRate,\n            recentFailures: flakyTest.recentFailures,\n            pattern: flakyTest.pattern,\n            severity: flakyTest.severity,\n            recommendation: flakyTest.recommendation,\n            lastSeen: new Date(flakyTest.lastSeen),\n            affectedBranches: flakyTest.affectedBranches,\n            updatedAt: new Date()\n          },\n          create: {\n            repositoryOwner: data.repository.owner,\n            repositoryName: data.repository.repo,\n            className: flakyTest.className,\n            testName: flakyTest.testName,\n            flakinessScore: flakyTest.flakinessScore,\n            totalRuns: flakyTest.totalRuns,\n            failures: flakyTest.failures,\n            successRate: flakyTest.successRate,\n            recentFailures: flakyTest.recentFailures,\n            pattern: flakyTest.pattern,\n            severity: flakyTest.severity,\n            recommendation: flakyTest.recommendation,\n            firstSeen: new Date(flakyTest.firstSeen),\n            lastSeen: new Date(flakyTest.lastSeen),\n            affectedBranches: flakyTest.affectedBranches,\n            createdAt: new Date(),\n            updatedAt: new Date()\n          }\n        });\n      }\n      \n      // Create analysis record\n      await tx.flakinessAnalysis.create({\n        data: {\n          workflowRunId: data.workflowRunId,\n          repositoryOwner: data.repository.owner,\n          repositoryName: data.repository.repo,\n          analyzedTests: flakyTests.length > 0 ? flakyTests[0].totalRuns : 0,\n          flakyTests: flakyTests.length,\n          overallFlakinessScore: calculateOverallFlakinessScore(flakyTests),\n          createdAt: new Date()\n        }\n      });\n    });\n    \n    logger.info({\n      repository: `${data.repository.owner}/${data.repository.repo}`,\n      workflowRunId: data.workflowRunId,\n      flakyTestsStored: flakyTests.length\n    }, 'Analysis results stored successfully');\n    \n  } catch (error) {\n    logger.error({\n      repository: `${data.repository.owner}/${data.repository.repo}`,\n      workflowRunId: data.workflowRunId,\n      error: error instanceof Error ? error.message : String(error)\n    }, 'Failed to store analysis results');\n    throw error;\n  }\n}\n\n// ============================================================================\n// GitHub Integration Functions\n// ============================================================================\n\n/**\n * Update GitHub Check Run with analysis results\n */\nasync function updateGitHubCheckRun(\n  github: Octokit,\n  data: RunsAnalyzeJobData,\n  flakyTests: FlakyTestResult[]\n): Promise<boolean> {\n  try {\n    const startTime = Date.now();\n    \n    // Find existing check run or create new one\n    const checkRunName = 'FlakeGuard Analysis';\n    \n    // Get workflow run details\n    const workflowRunResponse = await github.rest.actions.getWorkflowRun({\n      owner: data.repository.owner,\n      repo: data.repository.repo,\n      run_id: data.workflowRunId\n    });\n    \n    const workflowRun = workflowRunResponse.data;\n    const headSha = workflowRun.head_sha;\n    \n    // Create check run summary\n    const summary = createCheckRunSummary(flakyTests);\n    const conclusion = flakyTests.some(t => t.severity === 'critical') ? 'failure' : 'success';\n    \n    // Create or update check run\n    const checkRunResponse = await github.rest.checks.create({\n      owner: data.repository.owner,\n      repo: data.repository.repo,\n      name: checkRunName,\n      head_sha: headSha,\n      status: 'completed',\n      conclusion,\n      output: {\n        title: `FlakeGuard Analysis Results`,\n        summary,\n        text: createDetailedReport(flakyTests)\n      }\n    });\n    \n    const duration = Date.now() - startTime;\n    recordGitHubApiCall('createCheckRun', 'POST', checkRunResponse.status, duration);\n    \n    logger.info({\n      repository: `${data.repository.owner}/${data.repository.repo}`,\n      workflowRunId: data.workflowRunId,\n      checkRunId: checkRunResponse.data.id,\n      conclusion\n    }, 'GitHub Check Run updated successfully');\n    \n    return true;\n    \n  } catch (error) {\n    logger.error({\n      repository: `${data.repository.owner}/${data.repository.repo}`,\n      workflowRunId: data.workflowRunId,\n      error: error instanceof Error ? error.message : String(error)\n    }, 'Failed to update GitHub Check Run');\n    return false;\n  }\n}\n\n/**\n * Create check run summary\n */\nfunction createCheckRunSummary(flakyTests: FlakyTestResult[]): string {\n  if (flakyTests.length === 0) {\n    return '✅ No flaky tests detected in this run.';\n  }\n  \n  const criticalCount = flakyTests.filter(t => t.severity === 'critical').length;\n  const highCount = flakyTests.filter(t => t.severity === 'high').length;\n  const mediumCount = flakyTests.filter(t => t.severity === 'medium').length;\n  const lowCount = flakyTests.filter(t => t.severity === 'low').length;\n  \n  let summary = `🔍 **Flaky Test Analysis Results**\\n\\n`;\n  summary += `Found ${flakyTests.length} potentially flaky test${flakyTests.length === 1 ? '' : 's'}:\\n\\n`;\n  \n  if (criticalCount > 0) summary += `🚨 ${criticalCount} Critical\\n`;\n  if (highCount > 0) summary += `⚠️ ${highCount} High\\n`;\n  if (mediumCount > 0) summary += `⚡ ${mediumCount} Medium\\n`;\n  if (lowCount > 0) summary += `ℹ️ ${lowCount} Low\\n`;\n  \n  return summary;\n}\n\n/**\n * Create detailed report\n */\nfunction createDetailedReport(flakyTests: FlakyTestResult[]): string {\n  if (flakyTests.length === 0) {\n    return 'All tests appear to be stable. No flakiness detected.';\n  }\n  \n  let report = '## Detailed Flakiness Report\\n\\n';\n  \n  // Group by severity\n  const bySeverity = {\n    critical: flakyTests.filter(t => t.severity === 'critical'),\n    high: flakyTests.filter(t => t.severity === 'high'),\n    medium: flakyTests.filter(t => t.severity === 'medium'),\n    low: flakyTests.filter(t => t.severity === 'low')\n  };\n  \n  for (const [severity, tests] of Object.entries(bySeverity)) {\n    if (tests.length === 0) continue;\n    \n    const icon = {\n      critical: '🚨',\n      high: '⚠️',\n      medium: '⚡',\n      low: 'ℹ️'\n    }[severity];\n    \n    report += `### ${icon} ${severity.toUpperCase()} Priority Tests\\n\\n`;\n    \n    for (const test of tests.slice(0, 10)) { // Limit to top 10 per severity\n      report += `**${test.className}.${test.testName}**\\n`;\n      report += `- Flakiness Score: ${(test.flakinessScore * 100).toFixed(1)}%\\n`;\n      report += `- Success Rate: ${(test.successRate * 100).toFixed(1)}% (${test.failures}/${test.totalRuns} failures)\\n`;\n      report += `- Recent Failures: ${test.recentFailures}\\n`;\n      report += `- Pattern: ${test.pattern}\\n`;\n      report += `- Recommendation: ${test.recommendation}\\n\\n`;\n    }\n  }\n  \n  return report;\n}\n\n// ============================================================================\n// Utility Functions\n// ============================================================================\n\n/**\n * Generate high-level recommendations\n */\nfunction generateRecommendations(flakyTests: FlakyTestResult[]): string[] {\n  const recommendations: string[] = [];\n  \n  const criticalTests = flakyTests.filter(t => t.severity === 'critical');\n  if (criticalTests.length > 0) {\n    recommendations.push(`Quarantine ${criticalTests.length} critical flaky test${criticalTests.length === 1 ? '' : 's'} immediately`);\n  }\n  \n  const timingTests = flakyTests.filter(t => t.pattern === 'timing');\n  if (timingTests.length > 0) {\n    recommendations.push(`Review timing and synchronization in ${timingTests.length} test${timingTests.length === 1 ? '' : 's'}`);\n  }\n  \n  const envTests = flakyTests.filter(t => t.pattern === 'environmental');\n  if (envTests.length > 0) {\n    recommendations.push(`Investigate environmental dependencies for ${envTests.length} test${envTests.length === 1 ? '' : 's'}`);\n  }\n  \n  if (flakyTests.length > 10) {\n    recommendations.push('Consider implementing systematic test quality improvements');\n  }\n  \n  return recommendations;\n}\n\n/**\n * Create empty analysis result\n */\nfunction createEmptyAnalysisResult(workflowRunId: number, startTime: number): AnalysisResult {\n  return {\n    success: true,\n    workflowRunId,\n    analyzedTests: 0,\n    flakyTests: [],\n    overallFlakinessScore: 0,\n    recommendations: [],\n    checkRunUpdated: false,\n    processingTimeMs: Date.now() - startTime,\n    errors: [],\n    warnings: []\n  };\n}\n\n/**\n * Create mock GitHub client for testing\n */\nfunction createMockGitHubClient(): Octokit {\n  return {\n    rest: {\n      actions: {\n        getWorkflowRun: async () => ({\n          data: {\n            head_sha: 'mock-sha',\n            status: 'completed'\n          },\n          status: 200\n        })\n      },\n      checks: {\n        create: async () => ({\n          data: { id: 123 },\n          status: 201\n        })\n      }\n    }\n  } as any;\n}\n\n// ============================================================================\n// Export Processor Factory\n// ============================================================================\n\n/**\n * Factory function for runs analysis processor\n */\nexport function runsAnalyzeProcessor(\n  prisma: PrismaClient,\n  octokit?: Octokit\n) {\n  const processor = createRunsAnalyzeProcessor(prisma, octokit);\n  \n  return async (job: Job<RunsAnalyzeJobData>): Promise<AnalysisResult> => {\n    return processor(job);\n  };\n}\n\nexport default runsAnalyzeProcessor;"