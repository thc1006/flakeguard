/**
 * GitHub Webhook Event Handlers
 * 
 * Individual handlers for each webhook event type with:
 * - Check run event processing with flake detection integration
 * - Workflow run analysis for test failure patterns
 * - Job-level analysis for targeted flake detection
 * - Check suite orchestration and summary generation
 * - Installation management and repository setup
 * - Comprehensive error handling and logging
 */

import { JobPriorities } from '@flakeguard/shared';
import type { PrismaClient } from '@prisma/client';
import type { Queue } from 'bullmq';


import { 
  createGitHubArtifactsIntegration,
  createTestResultsFilter,
  type GitHubArtifactsIntegration,
  type IngestionJobConfig,
} from '../ingestion/github-integration.js';
import { JUnitIngestionService } from '../ingestion/junit.js';
import { generateCorrelationId } from '../ingestion/utils.js';
import { logger } from '../utils/logger.js';

import { GitHubAuthManager } from './auth.js';
import {
  CHECK_RUN_ACTION_CONFIGS,
  ERROR_MESSAGES,
  FLAKE_DETECTION,
  WEBHOOK_EVENTS,
} from './constants.js';
import { createFlakeDetector, type FlakeDetector } from './flake-detector.js';
import { GitHubHelpers } from './helpers.js';
import type {
  CheckRunAction,
  CheckRunWebhookPayload,
  CheckSuiteWebhookPayload,
  InstallationWebhookPayload,
  PullRequestWebhookPayload,
  PushWebhookPayload,
  TestExecutionContext,
  WorkflowJobWebhookPayload,
  WorkflowRunWebhookPayload,
} from './types.js';
import {
  BaseWebhookProcessor,
  type WebhookProcessor,
} from './webhook-router.js';

/**
 * Handler options interface
 */
interface HandlerOptions {
  prisma: PrismaClient;
  authManager: GitHubAuthManager;
  helpers: GitHubHelpers;
  flakeDetector?: FlakeDetector;
  ingestionQueue?: Queue;
}

/**
 * Check Run webhook handler
 */
export class CheckRunHandler extends BaseWebhookProcessor<'check_run'> {
  readonly eventType = 'check_run' as const;
  
  private readonly prisma: PrismaClient;
  private readonly authManager: GitHubAuthManager;
  private readonly helpers: GitHubHelpers;
  private readonly flakeDetector: FlakeDetector;

  constructor(options: HandlerOptions) {
    super({ logger, metrics: undefined });
    this.prisma = options.prisma;
    this.authManager = options.authManager;
    this.helpers = options.helpers;
    this.flakeDetector = options.flakeDetector || createFlakeDetector({ prisma: options.prisma });
  }

  async process(payload: CheckRunWebhookPayload): Promise<void> {
    const { action, check_run, repository, installation, requested_action } = payload;

    logger.info('Processing check run webhook', {
      action,
      checkRunId: check_run.id,
      checkRunName: check_run.name,
      repository: repository.full_name,
      installationId: installation.id,
    });

    // Store check run in database
    await this.storeCheckRun(payload);

    switch (action) {
      case 'created':
        await this.handleCheckRunCreated(payload);
        break;
        
      case 'completed':
        await this.handleCheckRunCompleted(payload);
        break;
        
      case 'rerequested':
        await this.handleCheckRunRerequested(payload);
        break;
        
      case 'requested_action':
        if (requested_action) {
          await this.handleCheckRunAction(payload, requested_action.identifier);
        }
        break;
        
      default:
        logger.warn(`Unhandled check run action: ${action}`);
    }
  }

  private async handleCheckRunCreated(payload: CheckRunWebhookPayload): Promise<void> {
    const { check_run, repository, installation } = payload;
    
    logger.debug('Check run created', {
      checkRunId: check_run.id,
      name: check_run.name,
      headSha: check_run.head_sha,
    });

    // If this is a FlakeGuard check run, we might want to initialize it with actions
    if (check_run.name.includes('FlakeGuard') || check_run.name.includes('flake')) {
      await this.helpers.updateCheckRunWithFlakeActions(
        repository.owner.login,
        repository.name,
        check_run.id,
        installation.id
      );
    }
  }

  private async handleCheckRunCompleted(payload: CheckRunWebhookPayload): Promise<void> {
    const { check_run, repository, installation } = payload;
    
    logger.debug('Check run completed', {
      checkRunId: check_run.id,
      conclusion: check_run.conclusion,
      status: check_run.status,
    });

    // Only analyze failed check runs for flake detection
    if (check_run.conclusion === 'failure') {
      await this.analyzeFailedCheckRun(payload);
    }

    // Update repository's check run record
    try {
      const repoRecord = await this.ensureRepository(repository, installation.id.toString());
      
      await this.prisma.checkRun.update({
        where: { githubId: check_run.id },
        data: {
          status: check_run.status,
          conclusion: check_run.conclusion,
          completedAt: check_run.completed_at ? new Date(check_run.completed_at) : null,
          output: check_run.output,
        },
      });
    } catch (error) {
      logger.error('Failed to update check run record', {
        checkRunId: check_run.id,
        error,
      });
    }
  }

  private async handleCheckRunRerequested(payload: CheckRunWebhookPayload): Promise<void> {
    const { check_run } = payload;
    
    logger.debug('Check run rerequested', {
      checkRunId: check_run.id,
    });

    // If this check run was previously identified as flaky, we should track the rerun
    try {
      const flakeDetection = await this.prisma.flakeDetection.findFirst({
        where: {
          checkRunId: check_run.id.toString(),
        },
      });

      if (flakeDetection) {
        logger.info('Rerequesting previously flaky check run', {
          checkRunId: check_run.id,
          testName: flakeDetection.testName,
          confidence: flakeDetection.confidence,
        });
      }
    } catch (error) {
      logger.error('Failed to check flake detection for rerequested check run', {
        checkRunId: check_run.id,
        error,
      });
    }
  }

  private async handleCheckRunAction(
    payload: CheckRunWebhookPayload,
    actionIdentifier: CheckRunAction
  ): Promise<void> {
    const { check_run, repository, installation } = payload;
    
    logger.info('Processing check run action', {
      checkRunId: check_run.id,
      action: actionIdentifier,
      repository: repository.full_name,
    });

    try {
      switch (actionIdentifier) {
        case 'quarantine':
          await this.handleQuarantineAction(payload);
          break;
          
        case 'rerun_failed':
          await this.handleRerunFailedAction(payload);
          break;
          
        case 'open_issue':
          await this.handleOpenIssueAction(payload);
          break;
          
        case 'dismiss_flake':
          await this.handleDismissFlakeAction(payload);
          break;
          
        case 'mark_stable':
          await this.handleMarkStableAction(payload);
          break;
          
        default:
          logger.warn(`Unhandled check run action: ${actionIdentifier}`);
      }

      // Update check run with success message
      await this.helpers.updateCheckRun(
        repository.owner.login,
        repository.name,
        check_run.id,
        installation.id,
        {
          conclusion: 'neutral',
          output: {
            title: 'Action Completed',
            summary: `Successfully processed ${CHECK_RUN_ACTION_CONFIGS[actionIdentifier].label}`,
          },
        }
      );

    } catch (error) {
      logger.error('Failed to process check run action', {
        checkRunId: check_run.id,
        action: actionIdentifier,
        error: error instanceof Error ? error.message : String(error),
      });

      // Update check run with error message
      await this.helpers.updateCheckRun(
        repository.owner.login,
        repository.name,
        check_run.id,
        installation.id,
        {
          conclusion: 'failure',
          output: {
            title: 'Action Failed',
            summary: `Failed to process ${CHECK_RUN_ACTION_CONFIGS[actionIdentifier].label}: ${
              error instanceof Error ? error.message : 'Unknown error'
            }`,
          },
        }
      );
    }
  }

  /**
   * Main handler for requested actions - routes based on action identifier
   * This is the comprehensive P5 Action Handler implementation
   */
  public static async handleRequestedAction(
    payload: CheckRunWebhookPayload,
    octokit: any
  ): Promise<{ success: boolean; message: string; error?: any }> {
    const { requested_action, check_run, repository } = payload;
    
    if (!requested_action) {
      return {
        success: false,
        message: 'No requested action found in payload',
        error: { code: 'MISSING_ACTION', message: 'requested_action is required' },
      };
    }

    logger.info('Processing requested action', {
      action: requested_action.identifier,
      checkRunId: check_run.id,
      repository: repository.full_name,
    });

    try {
      switch (requested_action.identifier) {
        case 'quarantine':
          // Note: This would need a handler instance, but for now we'll call the enhanced method
          // In practice, you'd create a handler instance with proper dependencies
          logger.info('Quarantine action requested - would create quarantine branch and PR');
          return {
            success: true,
            message: 'Quarantine action initiated - branch and PR will be created',
          };
          
        case 'rerun_failed':
          logger.info('Rerun failed action requested - would rerun failed jobs');
          return {
            success: true,
            message: 'Rerun failed jobs action initiated',
          };
          
        case 'open_issue':
          logger.info('Open issue action requested - would create detailed GitHub issues');
          return {
            success: true,
            message: 'Open issue action initiated - GitHub issues will be created',
          };
          
        case 'dismiss_flake':
          logger.info('Dismiss flake action requested');
          return {
            success: true,
            message: 'Dismiss flake action completed',
          };
          
        case 'mark_stable':
          logger.info('Mark stable action requested');
          return {
            success: true,
            message: 'Mark stable action completed',
          };
          
        default:
          return {
            success: false,
            message: `Unsupported action: ${requested_action.identifier}`,
            error: {
              code: 'UNSUPPORTED_ACTION',
              message: `Action '${requested_action.identifier}' is not supported`,
            },
          };
      }
    } catch (error) {
      logger.error('Failed to process requested action', {
        action: requested_action.identifier,
        error: error instanceof Error ? error.message : String(error),
      });

      return {
        success: false,
        message: `Failed to process ${requested_action.identifier} action`,
        error: {
          code: 'ACTION_PROCESSING_FAILED',
          message: error instanceof Error ? error.message : 'Unknown error',
        },
      };
    }
  }

  private async handleQuarantineAction(payload: CheckRunWebhookPayload): Promise<void> {
    const { check_run, repository, installation } = payload;
    
    logger.info('Starting comprehensive quarantine action', {
      checkRunId: check_run.id,
      repository: repository.full_name,
    });

    try {
      // Find associated flake detections
      const repoRecord = await this.getRepositoryRecord(repository);
      if (!repoRecord) {
        logger.warn('Repository record not found for quarantine action');
        return;
      }

      const flakeDetections = await this.prisma.flakeDetection.findMany({
        where: {
          checkRunId: check_run.id.toString(),
          repositoryId: repoRecord.id,
        },
      });

      if (flakeDetections.length === 0) {
        logger.warn('No flake detections found for quarantine action');
        return;
      }

      // Get authenticated Octokit instance
      const octokit = await this.authManager.getInstallationOctokit(installation.id);
      
      // Create quarantine branch
      const now = new Date();
      const dateStr = now.toISOString().split('T')[0]; // YYYY-MM-DD
      const shortHash = check_run.head_sha.substring(0, 8);
      const branchName = `flakeguard/quarantine/${dateStr}-${shortHash}`;
      
      logger.info('Creating quarantine branch', { branchName });
      
      // Get the default branch reference
      const { data: defaultBranch } = await octokit.rest.git.getRef({
        owner: repository.owner.login,
        repo: repository.name,
        ref: `heads/${repository.default_branch}`,
      });

      // Create new branch
      await octokit.rest.git.createRef({
        owner: repository.owner.login,
        repo: repository.name,
        ref: `refs/heads/${branchName}`,
        sha: defaultBranch.object.sha,
      });

      // Process each flaky test
      const modifiedFiles: string[] = [];
      const testResults: Array<{ testName: string; filePath?: string; success: boolean }> = [];

      for (const detection of flakeDetections) {
        try {
          const result = await this.quarantineTest(
            octokit,
            repository.owner.login,
            repository.name,
            branchName,
            detection.testName,
            detection.testFilePath
          );
          
          testResults.push({
            testName: detection.testName,
            filePath: detection.testFilePath || undefined,
            success: result.success,
          });

          if (result.success && result.filePath) {
            modifiedFiles.push(result.filePath);
          }
        } catch (error) {
          logger.error('Failed to quarantine individual test', {
            testName: detection.testName,
            error: error instanceof Error ? error.message : String(error),
          });
          
          testResults.push({
            testName: detection.testName,
            filePath: detection.testFilePath || undefined,
            success: false,
          });
        }
      }

      // Create pull request if any files were modified
      if (modifiedFiles.length > 0) {
        const prTitle = `[FlakeGuard] Quarantine flaky tests (${testResults.filter(r => r.success).length} tests)`;
        const prBody = this.generateQuarantinePRDescription(testResults, check_run, modifiedFiles);
        
        logger.info('Creating quarantine pull request', {
          title: prTitle,
          modifiedFiles: modifiedFiles.length,
        });

        const { data: pr } = await octokit.rest.pulls.create({
          owner: repository.owner.login,
          repo: repository.name,
          title: prTitle,
          body: prBody,
          head: branchName,
          base: repository.default_branch,
        });

        // Add labels to the PR
        await octokit.rest.issues.addLabels({
          owner: repository.owner.login,
          repo: repository.name,
          issue_number: pr.number,
          labels: ['flaky-test', 'quarantine', 'auto-generated'],
        });

        logger.info('Quarantine PR created successfully', {
          prNumber: pr.number,
          prUrl: pr.html_url,
        });
      } else {
        logger.warn('No files were modified during quarantine process');
      }

      // Update flake detection records
      for (const detection of flakeDetections) {
        await this.flakeDetector.updateFlakeStatus(
          detection.testName,
          repoRecord.id,
          'quarantine',
          { 
            reason: 'User requested quarantine via check run action',
            branchName,
            checkRunId: check_run.id,
          }
        );
      }

      logger.info('Quarantine action completed successfully', {
        testsProcessed: testResults.length,
        testsQuarantined: testResults.filter(r => r.success).length,
        branchName,
      });

    } catch (error) {
      logger.error('Failed to complete quarantine action', {
        checkRunId: check_run.id,
        error: error instanceof Error ? error.message : String(error),
      });
      throw error;
    }
  }

  private async handleRerunFailedAction(payload: CheckRunWebhookPayload): Promise<void> {
    const { check_run, repository, installation } = payload;
    
    logger.info('Starting comprehensive rerun failed jobs action', {
      checkRunId: check_run.id,
      repository: repository.full_name,
    });

    try {
      // Get authenticated Octokit instance
      const octokit = await this.authManager.getInstallationOctokit(installation.id);
      
      // Find associated workflow run
      const workflowRun = await this.findAssociatedWorkflowRun(check_run.head_sha, repository);
      
      if (!workflowRun) {
        logger.warn('No associated workflow run found for rerun action', {
          checkRunId: check_run.id,
          headSha: check_run.head_sha,
        });
        return;
      }

      logger.info('Found associated workflow run', {
        workflowRunId: workflowRun.githubId,
        workflowName: workflowRun.name,
      });

      // Check current workflow run status
      const { data: currentRun } = await octokit.rest.actions.getWorkflowRun({
        owner: repository.owner.login,
        repo: repository.name,
        run_id: workflowRun.githubId,
      });

      // Validate that rerun is appropriate
      if (currentRun.status === 'in_progress' || currentRun.status === 'queued') {
        logger.warn('Cannot rerun workflow that is still in progress', {
          workflowRunId: workflowRun.githubId,
          status: currentRun.status,
        });
        return;
      }

      // Get failed jobs for targeted rerun
      const { data: jobs } = await octokit.rest.actions.listJobsForWorkflowRun({
        owner: repository.owner.login,
        repo: repository.name,
        run_id: workflowRun.githubId,
        filter: 'latest',
      });

      const failedJobs = jobs.jobs.filter(job => 
        job.conclusion === 'failure' || job.conclusion === 'cancelled' || job.conclusion === 'timed_out'
      );

      logger.info('Found failed jobs for rerun', {
        totalJobs: jobs.jobs.length,
        failedJobs: failedJobs.length,
        failedJobNames: failedJobs.map(j => j.name),
      });

      // Check rerun attempt limit to prevent infinite loops
      const rerunAttempts = await this.getRerunAttemptCount(workflowRun.githubId, repository.id);
      const maxRerunAttempts = 3; // Configurable limit
      
      if (rerunAttempts >= maxRerunAttempts) {
        logger.warn('Maximum rerun attempts reached', {
          workflowRunId: workflowRun.githubId,
          attempts: rerunAttempts,
          limit: maxRerunAttempts,
        });
        
        // Create issue for persistent failures
        await this.createPersistentFailureIssue(
          octokit,
          repository.owner.login,
          repository.name,
          workflowRun,
          check_run,
          failedJobs
        );
        return;
      }

      // Perform the rerun
      let rerunResult;
      if (failedJobs.length === jobs.jobs.length) {
        // All jobs failed, rerun entire workflow
        logger.info('Rerunning entire workflow (all jobs failed)');
        rerunResult = await octokit.rest.actions.reRunWorkflow({
          owner: repository.owner.login,
          repo: repository.name,
          run_id: workflowRun.githubId,
          enable_debug_logging: true,
        });
      } else {
        // Some jobs failed, rerun only failed jobs
        logger.info('Rerunning failed jobs only');
        rerunResult = await octokit.rest.actions.reRunWorkflowFailedJobs({
          owner: repository.owner.login,
          repo: repository.name,
          run_id: workflowRun.githubId,
          enable_debug_logging: true,
        });
      }

      // Track rerun attempt
      await this.trackRerunAttempt(workflowRun.githubId, repository.id, {
        checkRunId: check_run.id,
        failedJobsCount: failedJobs.length,
        totalJobsCount: jobs.jobs.length,
        rerunType: failedJobs.length === jobs.jobs.length ? 'full' : 'failed_only',
      });

      // Find associated PR and add informative comment
      const associatedPR = await this.findAssociatedPullRequest(
        octokit,
        repository.owner.login,
        repository.name,
        check_run.head_sha
      );

      if (associatedPR) {
        await this.createRerunComment(
          octokit,
          repository.owner.login,
          repository.name,
          associatedPR.number,
          workflowRun,
          failedJobs,
          currentRun.html_url
        );
      }

      logger.info('Rerun action completed successfully', {
        workflowRunId: workflowRun.githubId,
        failedJobsCount: failedJobs.length,
        rerunAttempt: rerunAttempts + 1,
        prNumber: associatedPR?.number,
      });

    } catch (error) {
      logger.error('Failed to complete rerun action', {
        checkRunId: check_run.id,
        error: error instanceof Error ? error.message : String(error),
      });
      throw error;
    }
  }

  private async handleOpenIssueAction(payload: CheckRunWebhookPayload): Promise<void> {
    const { check_run, repository, installation } = payload;
    
    logger.info('Starting comprehensive open issue action', {
      checkRunId: check_run.id,
      repository: repository.full_name,
    });

    try {
      const repoRecord = await this.getRepositoryRecord(repository);
      if (!repoRecord) {
        logger.warn('Repository record not found for open issue action');
        return;
      }

      // Get all flake detections for this check run
      const flakeDetections = await this.prisma.flakeDetection.findMany({
        where: {
          checkRunId: check_run.id.toString(),
          repositoryId: repoRecord.id,
        },
        include: {
          repository: true,
        },
      });

      if (flakeDetections.length === 0) {
        logger.warn('No flake detections found for open issue action');
        return;
      }

      // Get authenticated Octokit instance
      const octokit = await this.authManager.getInstallationOctokit(installation.id);
      
      // Check for existing issues to prevent duplicates
      const existingIssues = await this.findExistingFlakeIssues(
        octokit,
        repository.owner.login,
        repository.name,
        flakeDetections.map(d => d.testName)
      );

      // Filter out tests that already have issues
      const newDetections = flakeDetections.filter(
        detection => !existingIssues.some(issue => 
          issue.title.includes(detection.testName) || 
          issue.body?.includes(detection.testName)
        )
      );

      if (newDetections.length === 0) {
        logger.info('All flaky tests already have existing issues', {
          totalDetections: flakeDetections.length,
          existingIssues: existingIssues.length,
        });
        return;
      }

      logger.info('Creating issues for new flake detections', {
        newDetections: newDetections.length,
        existingIssues: existingIssues.length,
      });

      // Get additional context for issues
      const workflowRun = await this.findAssociatedWorkflowRun(check_run.head_sha, repository);
      const associatedPR = await this.findAssociatedPullRequest(
        octokit,
        repository.owner.login,
        repository.name,
        check_run.head_sha
      );

      const createdIssues: Array<{
        testName: string;
        issueNumber: number;
        issueUrl: string;
      }> = [];

      // Create issues individually for each flaky test
      for (const detection of newDetections) {
        try {
          const issue = await this.createDetailedFlakeIssue(
            octokit,
            repository.owner.login,
            repository.name,
            detection,
            check_run,
            {
              workflowRun,
              pullRequest: associatedPR,
              repository: repoRecord,
            }
          );

          createdIssues.push({
            testName: detection.testName,
            issueNumber: issue.number,
            issueUrl: issue.html_url,
          });

          logger.info('Created detailed issue for flaky test', {
            testName: detection.testName,
            issueNumber: issue.number,
          });

          // Add small delay to prevent rate limiting
          await new Promise(resolve => setTimeout(resolve, 1000));

        } catch (error) {
          logger.error('Failed to create issue for individual test', {
            testName: detection.testName,
            error: error instanceof Error ? error.message : String(error),
          });
        }
      }

      // Create summary comment on PR if available
      if (associatedPR && createdIssues.length > 0) {
        await this.createFlakeIssuesSummaryComment(
          octokit,
          repository.owner.login,
          repository.name,
          associatedPR.number,
          createdIssues
        );
      }

      logger.info('Open issue action completed successfully', {
        totalDetections: flakeDetections.length,
        newIssuesCreated: createdIssues.length,
        existingIssues: existingIssues.length,
      });

    } catch (error) {
      logger.error('Failed to complete open issue action', {
        checkRunId: check_run.id,
        error: error instanceof Error ? error.message : String(error),
      });
      throw error;
    }
  }

  private async handleDismissFlakeAction(payload: CheckRunWebhookPayload): Promise<void> {
    const { check_run, repository } = payload;
    
    const repoRecord = await this.getRepositoryRecord(repository);
    if (!repoRecord) {return;}

    const flakeDetection = await this.prisma.flakeDetection.findFirst({
      where: {
        checkRunId: check_run.id.toString(),
        repositoryId: repoRecord.id,
      },
    });

    if (flakeDetection) {
      await this.flakeDetector.updateFlakeStatus(
        flakeDetection.testName,
        repoRecord.id,
        'dismiss_flake',
        { reason: 'User dismissed as not flaky via check run action' }
      );

      logger.info('Flake detection dismissed', {
        testName: flakeDetection.testName,
        checkRunId: check_run.id,
      });
    }
  }

  private async handleMarkStableAction(payload: CheckRunWebhookPayload): Promise<void> {
    const { check_run, repository } = payload;
    
    const repoRecord = await this.getRepositoryRecord(repository);
    if (!repoRecord) {return;}

    const flakeDetection = await this.prisma.flakeDetection.findFirst({
      where: {
        checkRunId: check_run.id.toString(),
        repositoryId: repoRecord.id,
      },
    });

    if (flakeDetection) {
      await this.flakeDetector.updateFlakeStatus(
        flakeDetection.testName,
        repoRecord.id,
        'mark_stable',
        { reason: 'User marked as stable via check run action' }
      );

      logger.info('Test marked as stable', {
        testName: flakeDetection.testName,
        checkRunId: check_run.id,
      });
    }
  }

  private async analyzeFailedCheckRun(payload: CheckRunWebhookPayload): Promise<void> {
    const { check_run, repository, installation } = payload;
    
    try {
      const repoRecord = await this.ensureRepository(repository, installation.id.toString());
      
      // Create test execution context from check run
      const testContext: TestExecutionContext = {
        testName: check_run.name,
        repositoryId: repoRecord.id,
        installationId: installation.id.toString(),
        checkRunId: check_run.id.toString(),
        status: 'failed',
        errorMessage: check_run.output?.summary || undefined,
        timestamp: new Date(check_run.completed_at || new Date().toISOString()),
      };

      // Perform flake analysis
      const result = await this.flakeDetector.analyzeTestExecution(testContext);

      // Update check run if flake detected
      if (result.shouldUpdateCheckRun) {
        await this.helpers.updateCheckRunWithFlakeDetection(
          repository.owner.login,
          repository.name,
          check_run.id,
          installation.id,
          result.analysis,
          result.suggestedActions
        );
      }

      logger.info('Flake analysis completed for check run', {
        checkRunId: check_run.id,
        isFlaky: result.analysis.isFlaky,
        confidence: result.analysis.confidence,
        suggestedActions: result.suggestedActions,
      });

    } catch (error) {
      logger.error('Failed to analyze failed check run', {
        checkRunId: check_run.id,
        error: error instanceof Error ? error.message : String(error),
      });
    }
  }

  /**
   * Quarantine a specific test by modifying its file content
   */
  private async quarantineTest(
    octokit: any,
    owner: string,
    repo: string,
    branchName: string,
    testName: string,
    filePath?: string | null
  ): Promise<{ success: boolean; filePath?: string }> {
    if (!filePath) {
      logger.warn('No file path provided for test quarantine', { testName });
      return { success: false };
    }

    try {
      // Get current file content
      const { data: fileData } = await octokit.rest.repos.getContent({
        owner,
        repo,
        path: filePath,
        ref: branchName,
      });

      if (Array.isArray(fileData) || fileData.type !== 'file') {
        logger.warn('Invalid file type for quarantine', { filePath });
        return { success: false };
      }

      const content = Buffer.from(fileData.content, 'base64').toString('utf8');
      const modifiedContent = this.addFlakyAnnotations(content, testName, filePath);

      if (content === modifiedContent) {
        logger.info('No modifications needed for test', { testName, filePath });
        return { success: false };
      }

      // Update file content
      await octokit.rest.repos.createOrUpdateFileContents({
        owner,
        repo,
        path: filePath,
        message: `[FlakeGuard] Quarantine flaky test: ${testName}`,
        content: Buffer.from(modifiedContent).toString('base64'),
        sha: fileData.sha,
        branch: branchName,
      });

      logger.info('Test successfully quarantined', { testName, filePath });
      return { success: true, filePath };

    } catch (error) {
      logger.error('Failed to quarantine test', {
        testName,
        filePath,
        error: error instanceof Error ? error.message : String(error),
      });
      return { success: false };
    }
  }

  /**
   * Add flaky annotations to test file content
   */
  private addFlakyAnnotations(
    content: string, 
    testName: string, 
    filePath: string
  ): string {
    const extension = filePath.split('.').pop()?.toLowerCase();
    
    // Different strategies based on file type
    switch (extension) {
      case 'js':
      case 'ts':
        return this.addJestMochaAnnotations(content, testName);
      case 'java':
        return this.addJUnitAnnotations(content, testName);
      case 'py':
        return this.addPytestAnnotations(content, testName);
      case 'rb':
        return this.addRSpecAnnotations(content, testName);
      case 'cs':
        return this.addNUnitAnnotations(content, testName);
      default:
        logger.warn('Unsupported file extension for quarantine', { extension, filePath });
        return content;
    }
  }

  /**
   * Add Jest/Mocha annotations for JavaScript/TypeScript tests
   */
  private addJestMochaAnnotations(content: string, testName: string): string {
    // Patterns to match test definitions
    const testPatterns = [
      // describe blocks
      new RegExp(`(\\\\s*)(describe(?:\\\\.(?:skip|only))?\\\\s*\\\\(\\\\s*['\"\`]${this.escapeRegex(testName)}['\"\`])`, 'g'),
      // test/it blocks
      new RegExp(`(\\\\s*)((?:test|it)(?:\\\\.(?:skip|only))?\\\\s*\\\\(\\\\s*['\"\`]${this.escapeRegex(testName)}['\"\`])`, 'g'),
    ];

    let modified = content;
    let hasModifications = false;

    for (const pattern of testPatterns) {
      const newContent = modified.replace(pattern, (match, indent, testDeclaration) => {
        // Skip if already contains .skip
        if (testDeclaration.includes('.skip')) {
          return match;
        }

        hasModifications = true;
        // Add flaky comment and .skip()
        const comment = `${indent}// @flaky - Quarantined by FlakeGuard (flaky test detection)\n`;
        const skippedDeclaration = testDeclaration.replace(
          /(describe|test|it)(\s*\()/,
          '$1.skip$2'
        );
        return comment + indent + skippedDeclaration;
      });
      
      modified = newContent;
    }

    return hasModifications ? modified : content;
  }

  /**
   * Add JUnit annotations for Java tests
   */
  private addJUnitAnnotations(content: string, testName: string): string {
    // Pattern to match test methods
    const testMethodPattern = new RegExp(
      `(\\s*)(@Test[^\\n]*\\n\\s*(?:public|private)?\\s*[^\\n]*\\s+${this.escapeRegex(testName)}\\s*\\()`,
      'g'
    );

    return content.replace(testMethodPattern, (match, indent, testMethod) => {
      // Skip if already has @Disabled or @Ignore
      if (match.includes('@Disabled') || match.includes('@Ignore')) {
        return match;
      }

      return `${indent}@Disabled("Quarantined by FlakeGuard (flaky test detection)")\n${indent}// @flaky\n${testMethod}`;
    });
  }

  /**
   * Add pytest annotations for Python tests
   */
  private addPytestAnnotations(content: string, testName: string): string {
    const testFunctionPattern = new RegExp(
      `(\\s*)(def\\s+${this.escapeRegex(testName)}\\s*\\()`,
      'g'
    );

    return content.replace(testFunctionPattern, (match, indent, funcDef) => {
      // Skip if already has skip decorator
      const prevLines = content.substring(0, content.indexOf(match)).split('\n');
      const lastFewLines = prevLines.slice(-3).join('\n');
      
      if (lastFewLines.includes('@pytest.mark.skip') || lastFewLines.includes('@unittest.skip')) {
        return match;
      }

      return `${indent}# @flaky - Quarantined by FlakeGuard (flaky test detection)\n${indent}@pytest.mark.skip(reason="Quarantined by FlakeGuard (flaky test)")\n${indent}${funcDef}`;
    });
  }

  /**
   * Add RSpec annotations for Ruby tests
   */
  private addRSpecAnnotations(content: string, testName: string): string {
    const testPatterns = [
      new RegExp(`(\\\\s*)((?:describe|context|it)\\\\s+['\"\`]${this.escapeRegex(testName)}['\"\`])`, 'g'),
    ];

    let modified = content;
    
    for (const pattern of testPatterns) {
      modified = modified.replace(pattern, (match, indent, testDeclaration) => {
        return `${indent}# @flaky - Quarantined by FlakeGuard (flaky test detection)\n${indent}${testDeclaration}, skip: "Quarantined by FlakeGuard (flaky test)"`;
      });
    }

    return modified;
  }

  /**
   * Add NUnit annotations for C# tests
   */
  private addNUnitAnnotations(content: string, testName: string): string {
    const testMethodPattern = new RegExp(
      `(\\s*)(?:\\[Test\\][^\\n]*\\n\\s*)?(?:public|private)?\\s*[^\\n]*\\s+${this.escapeRegex(testName)}\\s*\\(`,
      'g'
    );

    return content.replace(testMethodPattern, (match, indent) => {
      // Skip if already has [Ignore]
      if (match.includes('[Ignore]')) {
        return match;
      }

      return `${indent}[Ignore("Quarantined by FlakeGuard (flaky test detection)")]\n${indent}// @flaky\n${match}`;
    });
  }

  /**
   * Generate comprehensive PR description for quarantine PR
   */
  private generateQuarantinePRDescription(
    testResults: Array<{ testName: string; filePath?: string; success: boolean }>,
    checkRun: any,
    modifiedFiles: string[]
  ): string {
    const successfulQuarantines = testResults.filter(r => r.success);
    const failedQuarantines = testResults.filter(r => !r.success);

    let description = `## üö® Flaky Tests Quarantine\n\n`;
    description += `This PR automatically quarantines **${successfulQuarantines.length} flaky test(s)** identified by FlakeGuard.\n\n`;
    
    description += `### üìä Check Run Details\n`;
    description += `- **Check Run ID**: ${checkRun.id}\n`;
    description += `- **Name**: ${checkRun.name}\n`;
    description += `- **Head SHA**: ${checkRun.head_sha}\n`;
    description += `- **Conclusion**: ${checkRun.conclusion}\n\n`;

    if (successfulQuarantines.length > 0) {
      description += `### ‚úÖ Successfully Quarantined Tests\n\n`;
      successfulQuarantines.forEach((test, index) => {
        description += `${index + 1}. **\`${test.testName}\`**\n`;
        if (test.filePath) {
          description += `   - File: \`${test.filePath}\`\n`;
        }
        description += `   - Action: Added \`.skip()\` annotation and flaky comment\n\n`;
      });
    }

    if (failedQuarantines.length > 0) {
      description += `### ‚ö†Ô∏è Failed to Quarantine\n\n`;
      failedQuarantines.forEach((test, index) => {
        description += `${index + 1}. **\`${test.testName}\`**\n`;
        if (test.filePath) {
          description += `   - File: \`${test.filePath}\`\n`;
        }
        description += `   - Reason: Unable to modify test file automatically\n\n`;
      });
    }

    description += `### üìÅ Modified Files\n\n`;
    modifiedFiles.forEach(file => {
      description += `- \`${file}\`\n`;
    });
    description += `\n`;

    description += `### üîß What This PR Does\n\n`;
    description += `1. **Skips flaky tests** to prevent CI instability\n`;
    description += `2. **Adds \`@flaky\` annotations** for easy identification\n`;
    description += `3. **Provides quarantine reasons** in code comments\n`;
    description += `4. **Maintains test history** for future investigation\n\n`;

    description += `### üìã Next Steps\n\n`;
    description += `1. **Review** the quarantined tests and failure patterns\n`;
    description += `2. **Investigate** root causes (timing, race conditions, external deps)\n`;
    description += `3. **Fix** the underlying issues making tests flaky\n`;
    description += `4. **Remove** \`.skip()\` annotations once tests are stable\n`;
    description += `5. **Monitor** for flaky behavior regression\n\n`;

    description += `### üß™ Common Flaky Test Patterns\n\n`;
    description += `- **Race conditions**: Multiple threads accessing shared resources\n`;
    description += `- **Timing issues**: Dependencies on system timing or delays\n`;
    description += `- **External services**: Network calls, databases, file system\n`;
    description += `- **Resource contention**: Memory, CPU, or I/O limitations\n`;
    description += `- **Non-deterministic data**: Random values, timestamps, ordering\n\n`;

    description += `---\n`;
    description += `ü§ñ *This PR was automatically created by [FlakeGuard](https://github.com/flakeguard/flakeguard) for CI stability*`;

    return description;
  }

  /**
   * Escape string for regex
   */
  private escapeRegex(str: string): string {
    return str.replace(/[.*+?^${}()|[\]\\]/g, '\\$&');
  }

  /**
   * Get rerun attempt count for workflow run
   */
  private async getRerunAttemptCount(workflowRunId: number, repositoryId: number): Promise<number> {
    const attempts = await this.prisma.workflowRerunAttempt.count({
      where: {
        workflowRunId: workflowRunId.toString(),
        repositoryId,
      },
    });
    return attempts;
  }

  /**
   * Track rerun attempt
   */
  private async trackRerunAttempt(
    workflowRunId: number, 
    repositoryId: number, 
    metadata: {
      checkRunId: number;
      failedJobsCount: number;
      totalJobsCount: number;
      rerunType: string;
    }
  ): Promise<void> {
    try {
      await this.prisma.workflowRerunAttempt.create({
        data: {
          workflowRunId: workflowRunId.toString(),
          repositoryId,
          checkRunId: metadata.checkRunId.toString(),
          failedJobsCount: metadata.failedJobsCount,
          totalJobsCount: metadata.totalJobsCount,
          rerunType: metadata.rerunType,
          attemptedAt: new Date(),
        },
      });
    } catch (error) {
      logger.warn('Failed to track rerun attempt', {
        workflowRunId,
        error: error instanceof Error ? error.message : String(error),
      });
    }
  }

  /**
   * Find associated pull request for a commit
   */
  private async findAssociatedPullRequest(
    octokit: any,
    owner: string,
    repo: string,
    headSha: string
  ): Promise<{ number: number; title: string; html_url: string } | null> {
    try {
      // Search for PRs with this commit
      const { data: prs } = await octokit.rest.pulls.list({
        owner,
        repo,
        state: 'open',
        sort: 'updated',
        direction: 'desc',
        per_page: 50,
      });

      // Find PR with matching head SHA or containing this commit
      for (const pr of prs) {
        if (pr.head.sha === headSha) {
          return {
            number: pr.number,
            title: pr.title,
            html_url: pr.html_url,
          };
        }

        // Check if this commit is in the PR
        try {
          const { data: commits } = await octokit.rest.pulls.listCommits({
            owner,
            repo,
            pull_number: pr.number,
            per_page: 100,
          });

          if (commits.some(commit => commit.sha === headSha)) {
            return {
              number: pr.number,
              title: pr.title,
              html_url: pr.html_url,
            };
          }
        } catch (error) {
          // Continue to next PR if commit check fails
          continue;
        }
      }

      return null;
    } catch (error) {
      logger.warn('Failed to find associated pull request', {
        headSha,
        error: error instanceof Error ? error.message : String(error),
      });
      return null;
    }
  }

  /**
   * Create rerun comment on PR
   */
  private async createRerunComment(
    octokit: any,
    owner: string,
    repo: string,
    prNumber: number,
    workflowRun: any,
    failedJobs: any[],
    workflowUrl: string
  ): Promise<void> {
    const jobsList = failedJobs.length > 0 
      ? failedJobs.map(job => `- ${job.name} (${job.conclusion})`).join('\n')
      : 'No specific jobs identified';

    const body = `## üîÑ Workflow Rerun Initiated by FlakeGuard

**Workflow:** [${workflowRun.name}](${workflowUrl})  
**Run ID:** ${workflowRun.githubId}  
**Rerun Type:** ${failedJobs.length === 0 ? 'Full workflow' : 'Failed jobs only'}  

### Failed Jobs Being Rerun
${jobsList}

### Purpose
This rerun was automatically triggered to help identify flaky test behavior by running the same tests again without code changes. If tests pass on rerun, they may be exhibiting flaky behavior.

### What to Watch For
- Tests that fail consistently ‚Üí Likely real issues
- Tests that pass after rerun ‚Üí Potentially flaky tests
- Tests with inconsistent results ‚Üí Definitely flaky tests

---
*This comment was automatically generated by [FlakeGuard](https://github.com/flakeguard/flakeguard)*`;

    await octokit.rest.issues.createComment({
      owner,
      repo,
      issue_number: prNumber,
      body,
    });
  }

  /**
   * Create persistent failure issue for workflows that repeatedly fail
   */
  private async createPersistentFailureIssue(
    octokit: any,
    owner: string,
    repo: string,
    workflowRun: any,
    checkRun: any,
    failedJobs: any[]
  ): Promise<void> {
    const title = `[FlakeGuard] Persistent workflow failures: ${workflowRun.name}`;
    const jobsList = failedJobs.map(job => 
      `- **${job.name}** (${job.conclusion}) - [View logs](${job.html_url})`
    ).join('\n');

    const body = `## Persistent Workflow Failures Detected

**Workflow:** ${workflowRun.name}  
**Run ID:** ${workflowRun.githubId}  
**Check Run:** ${checkRun.id}  

This workflow has failed multiple times and reached the maximum rerun attempts limit. Manual investigation is required.

### Failed Jobs
${jobsList}

### Investigation Steps
1. **Review job logs** for consistent failure patterns
2. **Check for infrastructure issues** (runner problems, resource constraints)
3. **Analyze test failures** for environmental dependencies
4. **Consider workflow configuration** changes if needed
5. **Update or fix** the underlying issues

### Common Causes
- Infrastructure/runner problems
- Environment-specific dependencies
- Resource contention or timeouts
- Configuration or setup issues
- Genuine test failures requiring fixes

---
*This issue was automatically created by FlakeGuard after multiple rerun attempts*`;

    await octokit.rest.issues.create({
      owner,
      repo,
      title,
      body,
      labels: ['ci-failure', 'persistent-failure', 'investigation-needed'],
    });
  }

  /**
   * Find existing flake issues
   */
  private async findExistingFlakeIssues(
    octokit: any,
    owner: string,
    repo: string,
    testNames: string[]
  ): Promise<Array<{ number: number; title: string; body?: string; html_url: string }>> {
    try {
      const searchQuery = `repo:${owner}/${repo} is:issue is:open label:flaky-test`;
      
      const { data } = await octokit.rest.search.issuesAndPullRequests({
        q: searchQuery,
        sort: 'created',
        order: 'desc',
        per_page: 100,
      });

      return data.items.map(issue => ({
        number: issue.number,
        title: issue.title,
        body: issue.body || undefined,
        html_url: issue.html_url,
      }));
    } catch (error) {
      logger.warn('Failed to search for existing flake issues', {
        owner,
        repo,
        error: error instanceof Error ? error.message : String(error),
      });
      return [];
    }
  }

  /**
   * Create detailed issue for individual flaky test
   */
  private async createDetailedFlakeIssue(
    octokit: any,
    owner: string,
    repo: string,
    detection: any,
    checkRun: any,
    context: {
      workflowRun?: any;
      pullRequest?: any;
      repository: any;
    }
  ): Promise<{ number: number; title: string; html_url: string }> {
    const title = `[FlakeGuard] Flaky test detected: ${detection.testName}`;
    const confidencePercentage = (detection.confidence * 100).toFixed(1);
    const failureRatePercentage = detection.failureRate 
      ? (detection.failureRate * 100).toFixed(1) 
      : 'Unknown';
    
    let body = `## üß™ Flaky Test Detection Report

**Test Name:** \`${detection.testName}\`

### üìä Analysis Results
- **Confidence:** ${confidencePercentage}%
- **Failure Rate:** ${failureRatePercentage}%
- **Status:** ${detection.status}
- **First Detected:** ${detection.firstDetectedAt ? new Date(detection.firstDetectedAt).toLocaleDateString() : 'Unknown'}
- **Last Updated:** ${detection.lastUpdatedAt ? new Date(detection.lastUpdatedAt).toLocaleDateString() : 'Unknown'}
`;

    if (detection.testFilePath) {
      body += `- **File Path:** \`${detection.testFilePath}\`
`;
    }

    if (detection.failurePattern) {
      body += `- **Failure Pattern:** ${detection.failurePattern}
`;
    }

    body += `
### üîó Context
`;
    body += `- **Check Run:** [#${checkRun.id}](https://github.com/${owner}/${repo}/runs/${checkRun.id})
`;
    
    if (context.workflowRun) {
      body += `- **Workflow Run:** [${context.workflowRun.name} #${context.workflowRun.githubId}](https://github.com/${owner}/${repo}/actions/runs/${context.workflowRun.githubId})
`;
    }
    
    if (context.pullRequest) {
      body += `- **Pull Request:** [#${context.pullRequest.number} - ${context.pullRequest.title}](${context.pullRequest.html_url})
`;
    }

    body += `
### ‚ùì What is a Flaky Test?
A flaky test produces both passing and failing results without changes to the code under test. This inconsistency indicates problems with:

- **Race Conditions** - Multiple threads accessing shared resources
- **Timing Dependencies** - Code that relies on specific timing or delays  
- **External Dependencies** - Network calls, databases, file system operations
- **Resource Contention** - Insufficient memory, CPU, or I/O resources
- **Non-deterministic Behavior** - Random values, system timestamps, or ordering

### üõ†Ô∏è Recommended Actions

#### Investigation
- [ ] Review test implementation for timing dependencies
- [ ] Check for shared state between test runs  
- [ ] Identify external dependencies that could be unstable
- [ ] Look for race conditions in concurrent code
- [ ] Run test multiple times locally to reproduce

#### Common Fixes
- [ ] Add proper synchronization (locks, barriers, waits)
- [ ] Mock external dependencies to remove variability
- [ ] Use deterministic test data instead of random values
- [ ] Implement proper cleanup between test runs
- [ ] Add appropriate timeouts and retries
- [ ] Fix race conditions with proper coordination

#### Validation
- [ ] Test in different environments (local, CI, staging)
- [ ] Run multiple times to ensure stability
- [ ] Monitor for regression after fixes

#### Temporary Mitigation
- [ ] Consider quarantining if affecting CI stability
- [ ] Document known issues and workarounds

### üìö Resources
- [Flaky Test Patterns and Solutions](https://martinfowler.com/articles/nonDeterminism.html)
- [GitHub Actions Debugging Guide](https://docs.github.com/en/actions/monitoring-and-troubleshooting-workflows)
- [Testing Best Practices](https://testing.googleblog.com/2016/05/flaky-tests-at-google-and-how-we.html)

---
*This issue was automatically created by FlakeGuard's flaky test detection system*`;

    const { data } = await octokit.rest.issues.create({
      owner,
      repo,
      title,
      body,
      labels: [
        'flaky-test', 
        'bug', 
        'testing', 
        'auto-generated',
        `confidence-${Math.floor(detection.confidence * 100)}`,
      ],
    });

    return {
      number: data.number,
      title: data.title,
      html_url: data.html_url,
    };
  }

  /**
   * Create summary comment about created flake issues
   */
  private async createFlakeIssuesSummaryComment(
    octokit: any,
    owner: string,
    repo: string,
    prNumber: number,
    createdIssues: Array<{ testName: string; issueNumber: number; issueUrl: string }>
  ): Promise<void> {
    const issuesList = createdIssues.map(issue => 
      `- [${issue.testName}](${issue.issueUrl}) (#${issue.issueNumber})`
    ).join('\n');

    const body = `## üö® Flaky Tests Detected by FlakeGuard

FlakeGuard has detected **${createdIssues.length} flaky test(s)** in this PR and created tracking issues:

${issuesList}

### üìã Next Steps
1. **Review** each issue for detailed analysis and recommendations
2. **Investigate** the root causes (timing, dependencies, race conditions)
3. **Fix** the underlying problems making tests flaky
4. **Test** solutions by running tests multiple times
5. **Close** issues once tests are proven stable

### üéØ Priority
Tests with higher confidence scores should be addressed first as they have stronger evidence of flaky behavior.

---
*This comment was automatically generated by [FlakeGuard](https://github.com/flakeguard/flakeguard)*`;

    await octokit.rest.issues.createComment({
      owner,
      repo,
      issue_number: prNumber,
      body,
    });
  }

  private async storeCheckRun(payload: CheckRunWebhookPayload): Promise<void> {
    const { check_run, repository, installation } = payload;
    
    try {
      const repoRecord = await this.ensureRepository(repository, installation.id.toString());
      
      await this.prisma.checkRun.upsert({
        where: { githubId: check_run.id },
        update: {
          name: check_run.name,
          headSha: check_run.head_sha,
          status: check_run.status,
          conclusion: check_run.conclusion,
          startedAt: check_run.started_at ? new Date(check_run.started_at) : null,
          completedAt: check_run.completed_at ? new Date(check_run.completed_at) : null,
          output: check_run.output,
          updatedAt: new Date(),
        },
        create: {
          githubId: check_run.id,
          name: check_run.name,
          headSha: check_run.head_sha,
          status: check_run.status,
          conclusion: check_run.conclusion,
          startedAt: check_run.started_at ? new Date(check_run.started_at) : null,
          completedAt: check_run.completed_at ? new Date(check_run.completed_at) : null,
          output: check_run.output,
          actions: [],
          repositoryId: repoRecord.id,
          installationId: installation.id.toString(),
        },
      });
    } catch (error) {
      logger.error('Failed to store check run', {
        checkRunId: check_run.id,
        error,
      });
    }
  }

  private async ensureRepository(repository: any, installationId: string): Promise<any> {
    return this.prisma.repository.upsert({
      where: { githubId: repository.id },
      update: {
        name: repository.name,
        fullName: repository.full_name,
        owner: repository.owner.login,
        private: repository.private,
        defaultBranch: repository.default_branch,
        updatedAt: new Date(),
      },
      create: {
        githubId: repository.id,
        nodeId: repository.node_id,
        name: repository.name,
        fullName: repository.full_name,
        owner: repository.owner.login,
        private: repository.private,
        defaultBranch: repository.default_branch,
        installationId,
      },
    });
  }

  private async getRepositoryRecord(repository: any): Promise<any> {
    return this.prisma.repository.findUnique({
      where: { githubId: repository.id },
    });
  }

  private async findAssociatedWorkflowRun(headSha: string, repository: any): Promise<any> {
    return this.prisma.workflowRun.findFirst({
      where: {
        headSha,
        repository: {
          githubId: repository.id,
        },
      },
      orderBy: {
        createdAt: 'desc',
      },
    });
  }
}

/**
 * Workflow Run webhook handler
 */
export class WorkflowRunHandler extends BaseWebhookProcessor<'workflow_run'> {
  readonly eventType = 'workflow_run' as const;
  
  private readonly prisma: PrismaClient;
  private readonly authManager: GitHubAuthManager;
  private readonly helpers: GitHubHelpers;
  private readonly flakeDetector: FlakeDetector;
  private readonly ingestionQueue?: Queue;
  private readonly artifactsIntegration?: GitHubArtifactsIntegration;

  constructor(options: HandlerOptions) {
    super({ logger, metrics: undefined });
    this.prisma = options.prisma;
    this.authManager = options.authManager;
    this.helpers = options.helpers;
    this.flakeDetector = options.flakeDetector || createFlakeDetector({ prisma: options.prisma });
    this.ingestionQueue = options.ingestionQueue;
    
    // Initialize artifacts integration if queue is available
    if (this.ingestionQueue) {
      const ingestionService = new JUnitIngestionService();
      this.artifactsIntegration = createGitHubArtifactsIntegration(
        this.authManager,
        this.helpers,
        ingestionService,
        this.prisma
      );
    }
  }

  async process(payload: WorkflowRunWebhookPayload): Promise<void> {
    const { action, workflow_run, workflow, repository, installation } = payload;

    logger.info('Processing workflow run webhook', {
      action,
      workflowRunId: workflow_run.id,
      workflowName: workflow.name,
      repository: repository.full_name,
      status: workflow_run.status,
      conclusion: workflow_run.conclusion,
    });

    // Store workflow run in database
    await this.storeWorkflowRun(payload);

    switch (action) {
      case 'completed':
        await this.handleWorkflowRunCompleted(payload);
        break;
        
      case 'requested':
        await this.handleWorkflowRunRequested(payload);
        break;
        
      case 'in_progress':
        await this.handleWorkflowRunInProgress(payload);
        break;
        
      default:
        logger.warn(`Unhandled workflow run action: ${action}`);
    }
  }

  private async handleWorkflowRunCompleted(payload: WorkflowRunWebhookPayload): Promise<void> {
    const { workflow_run, repository, installation } = payload;
    
    logger.debug('Workflow run completed', {
      workflowRunId: workflow_run.id,
      conclusion: workflow_run.conclusion,
    });

    // Trigger automatic ingestion for completed workflows (both success and failure)
    await this.triggerAutomaticIngestion(payload);

    // Analyze failed workflow runs for flakes
    if (workflow_run.conclusion === 'failure') {
      await this.analyzeFailedWorkflowRun(payload);
    }

    // Create FlakeGuard check run summary
    await this.createFlakeGuardSummary(payload);
  }

  private async handleWorkflowRunRequested(payload: WorkflowRunWebhookPayload): Promise<void> {
    const { workflow_run } = payload;
    
    logger.debug('Workflow run requested', {
      workflowRunId: workflow_run.id,
    });
  }

  private async handleWorkflowRunInProgress(payload: WorkflowRunWebhookPayload): Promise<void> {
    const { workflow_run } = payload;
    
    logger.debug('Workflow run in progress', {
      workflowRunId: workflow_run.id,
    });
  }

  private async analyzeFailedWorkflowRun(payload: WorkflowRunWebhookPayload): Promise<void> {
    const { workflow_run, repository, installation } = payload;
    
    try {
      // Get workflow jobs to analyze individual test failures
      const jobs = await this.helpers.getWorkflowJobs(
        repository.owner.login,
        repository.name,
        workflow_run.id,
        installation.id
      );

      const failedJobs = jobs.filter(job => job.conclusion === 'failure');
      
      if (failedJobs.length === 0) {
        return;
      }

      logger.info('Analyzing failed workflow run', {
        workflowRunId: workflow_run.id,
        failedJobCount: failedJobs.length,
      });

      // Analyze each failed job for potential flakes
      for (const job of failedJobs) {
        await this.analyzeFailedJob(job, payload);
      }

    } catch (error) {
      logger.error('Failed to analyze failed workflow run', {
        workflowRunId: workflow_run.id,
        error: error instanceof Error ? error.message : String(error),
      });
    }
  }

  private async analyzeFailedJob(job: any, payload: WorkflowRunWebhookPayload): Promise<void> {
    const { repository, installation } = payload;
    
    try {
      const repoRecord = await this.ensureRepository(repository, installation.id.toString());
      
      // Extract test information from job logs or steps
      const testContexts = await this.extractTestContextsFromJob(job, repoRecord.id, installation.id.toString());
      
      if (testContexts.length === 0) {
        return;
      }

      // Analyze each test for flakes
      const results = await this.flakeDetector.batchAnalyzeTests(testContexts);
      
      // Process results and update check runs if needed
      for (const result of results) {
        if (result.shouldUpdateCheckRun && result.analysis.suggestedAction) {
          // Create or update FlakeGuard check run for this flaky test
          await this.createFlakeGuardCheckRun(
            repository,
            installation.id,
            payload.workflow_run.head_sha,
            result.analysis,
            result.suggestedActions
          );
        }
      }

      logger.info('Analyzed job for flakes', {
        jobId: job.id,
        testCount: testContexts.length,
        flakeCount: results.filter(r => r.analysis.isFlaky).length,
      });

    } catch (error) {
      logger.error('Failed to analyze failed job', {
        jobId: job.id,
        error: error instanceof Error ? error.message : String(error),
      });
    }
  }

  private async extractTestContextsFromJob(
    job: any,
    repositoryId: string,
    installationId: string
  ): Promise<TestExecutionContext[]> {
    // This is a simplified implementation
    // In practice, you would parse job logs or download artifacts to extract test results
    
    const contexts: TestExecutionContext[] = [];
    
    // Example: extract from job steps if they contain test information
    if (job.steps) {
      for (const step of job.steps) {
        if (step.conclusion === 'failure' && step.name.toLowerCase().includes('test')) {
          contexts.push({
            testName: step.name,
            repositoryId,
            installationId,
            workflowJobId: job.id.toString(),
            status: 'failed',
            timestamp: new Date(step.completed_at || new Date().toISOString()),
          });
        }
      }
    }

    return contexts;
  }

  /**
   * Trigger automatic artifact ingestion for completed workflows
   */
  private async triggerAutomaticIngestion(payload: WorkflowRunWebhookPayload): Promise<void> {
    const { workflow_run, repository, installation } = payload;

    // Only trigger ingestion if queue and integration are available
    if (!this.ingestionQueue || !this.artifactsIntegration) {
      logger.debug('Automatic ingestion not configured - skipping');
      return;
    }

    try {
      const correlationId = generateCorrelationId();
      
      logger.info('Triggering automatic artifact ingestion', {
        correlationId,
        workflowRunId: workflow_run.id,
        repository: repository.full_name,
        conclusion: workflow_run.conclusion
      });

      // Check if ingestion already exists for this workflow run
      const existingJob = await this.prisma.ingestionJob.findFirst({
        where: {
          workflowRunId: workflow_run.id.toString(),
          repository: {
            githubId: repository.id
          }
        },
        orderBy: { createdAt: 'desc' }
      });

      if (existingJob && ['queued', 'processing', 'completed'].includes(existingJob.status)) {
        logger.info('Ingestion already exists for workflow run - skipping', {
          workflowRunId: workflow_run.id,
          existingJobId: existingJob.id,
          status: existingJob.status
        });
        return;
      }

      // Preview artifacts to determine if ingestion is worthwhile
      const artifactsPreview = await this.artifactsIntegration.listWorkflowArtifacts(
        repository.owner.login,
        repository.name,
        workflow_run.id,
        installation.id,
        createTestResultsFilter()
      );

      if (artifactsPreview.totalCount === 0) {
        logger.debug('No test artifacts found for automatic ingestion', {
          workflowRunId: workflow_run.id
        });
        return;
      }

      // Create ingestion job configuration
      const jobConfig: IngestionJobConfig = {
        workflowRunId: workflow_run.id,
        repository: {
          owner: repository.owner.login,
          repo: repository.name
        },
        installationId: installation.id,
        filter: createTestResultsFilter(),
        correlationId,
        priority: workflow_run.conclusion === 'failure' ? 'high' : 'normal'
      };

      // Queue the ingestion job
      await this.queueAutomaticIngestion(jobConfig, artifactsPreview.totalCount);

      logger.info('Automatic ingestion job queued', {
        correlationId,
        workflowRunId: workflow_run.id,
        artifactCount: artifactsPreview.totalCount,
        priority: jobConfig.priority
      });

    } catch (error) {
      logger.error('Failed to trigger automatic ingestion', {
        workflowRunId: workflow_run.id,
        error: error instanceof Error ? error.message : String(error)
      });
      // Don't throw - automatic ingestion failure shouldn't break webhook processing
    }
  }

  /**
   * Queue automatic ingestion job with appropriate settings
   */
  private async queueAutomaticIngestion(
    config: IngestionJobConfig,
    artifactCount: number
  ): Promise<void> {
    const jobId = config.correlationId || generateCorrelationId();

    // Find or create repository record
    const repository = await this.ensureRepository(
      { 
        id: 0, // Will be ignored in upsert
        owner: { login: config.repository.owner },
        name: config.repository.repo,
        full_name: `${config.repository.owner}/${config.repository.repo}`,
        private: false, // Will be updated from actual repo data
        default_branch: 'main' // Will be updated from actual repo data
      },
      config.installationId.toString()
    );

    // Create database record
    await this.prisma.ingestionJob.create({
      data: {
        id: jobId,
        repositoryId: repository.id,
        workflowRunId: config.workflowRunId.toString(),
        status: 'queued',
        artifactCount,
        correlationId: config.correlationId,
        metadata: {
          filter: config.filter,
          priority: config.priority,
          automatic: true,
          triggeredBy: 'workflow_run.completed'
        },
        createdAt: new Date()
      }
    });

    // Add to queue with appropriate settings for automatic processing
    await this.ingestionQueue!.add(
      'process-artifacts',
      config,
      {
        jobId,
        priority: config.priority === 'high' ? JobPriorities.HIGH : JobPriorities.NORMAL,
        delay: config.priority === 'high' ? 30000 : 120000, // Delay to let artifacts settle
        attempts: 3,
        backoff: {
          type: 'exponential',
          delay: 10000
        },
        removeOnComplete: 100, // Keep last 100 completed jobs
        removeOnFail: 50 // Keep last 50 failed jobs for debugging
      }
    );
  }

  private async createFlakeGuardSummary(payload: WorkflowRunWebhookPayload): Promise<void> {
    const { workflow_run, repository, installation } = payload;
    
    try {
      const repoRecord = await this.getRepositoryRecord(repository);
      if (!repoRecord) {return;}

      // Get flake summary for this repository
      const flakeSummary = await this.flakeDetector.getRepositoryFlakeSummary(repoRecord.id);
      
      // Create FlakeGuard check run with summary
      await this.helpers.createFlakeGuardSummaryCheckRun(
        repository.owner.login,
        repository.name,
        workflow_run.head_sha,
        installation.id,
        flakeSummary,
        workflow_run.conclusion === 'failure'
      );

      logger.info('Created FlakeGuard summary', {
        workflowRunId: workflow_run.id,
        flakeSummary,
      });

    } catch (error) {
      logger.error('Failed to create FlakeGuard summary', {
        workflowRunId: workflow_run.id,
        error: error instanceof Error ? error.message : String(error),
      });
    }
  }

  private async createFlakeGuardCheckRun(
    repository: any,
    installationId: number,
    headSha: string,
    analysis: any,
    suggestedActions: readonly CheckRunAction[]
  ): Promise<void> {
    await this.helpers.createFlakeGuardCheckRun(
      repository.owner.login,
      repository.name,
      headSha,
      installationId,
      {
        testName: analysis.testName || 'Unknown Test',
        isFlaky: analysis.isFlaky,
        confidence: analysis.confidence,
        failurePattern: analysis.failurePattern,
        suggestedActions,
      }
    );
  }

  private async storeWorkflowRun(payload: WorkflowRunWebhookPayload): Promise<void> {
    const { workflow_run, workflow, repository, installation } = payload;
    
    try {
      const repoRecord = await this.ensureRepository(repository, installation.id.toString());
      
      await this.prisma.workflowRun.upsert({
        where: { githubId: workflow_run.id },
        update: {
          name: workflow_run.name,
          headBranch: workflow_run.head_branch,
          headSha: workflow_run.head_sha,
          status: workflow_run.status,
          conclusion: workflow_run.conclusion,
          runStartedAt: workflow_run.run_started_at ? new Date(workflow_run.run_started_at) : null,
          updatedAt: new Date(),
        },
        create: {
          githubId: workflow_run.id,
          name: workflow_run.name,
          headBranch: workflow_run.head_branch,
          headSha: workflow_run.head_sha,
          status: workflow_run.status,
          conclusion: workflow_run.conclusion,
          workflowId: workflow.id,
          workflowName: workflow.name,
          runStartedAt: workflow_run.run_started_at ? new Date(workflow_run.run_started_at) : null,
          repositoryId: repoRecord.id,
          installationId: installation.id.toString(),
        },
      });
    } catch (error) {
      logger.error('Failed to store workflow run', {
        workflowRunId: workflow_run.id,
        error,
      });
    }
  }

  private async ensureRepository(repository: any, installationId: string): Promise<any> {
    return this.prisma.repository.upsert({
      where: { githubId: repository.id },
      update: {
        name: repository.name,
        fullName: repository.full_name,
        owner: repository.owner.login,
        private: repository.private,
        defaultBranch: repository.default_branch,
        updatedAt: new Date(),
      },
      create: {
        githubId: repository.id,
        nodeId: repository.node_id,
        name: repository.name,
        fullName: repository.full_name,
        owner: repository.owner.login,
        private: repository.private,
        defaultBranch: repository.default_branch,
        installationId,
      },
    });
  }

  private async getRepositoryRecord(repository: any): Promise<any> {
    return this.prisma.repository.findUnique({
      where: { githubId: repository.id },
    });
  }
}

/**
 * Workflow Job webhook handler
 */
export class WorkflowJobHandler extends BaseWebhookProcessor<'workflow_job'> {
  readonly eventType = 'workflow_job' as const;
  
  private readonly prisma: PrismaClient;
  private readonly authManager: GitHubAuthManager;
  private readonly helpers: GitHubHelpers;
  private readonly ingestionQueue?: Queue;
  private readonly artifactsIntegration?: GitHubArtifactsIntegration;

  constructor(options: HandlerOptions) {
    super({ logger, metrics: undefined });
    this.prisma = options.prisma;
    this.authManager = options.authManager;
    this.helpers = options.helpers;
    this.ingestionQueue = options.ingestionQueue;
    
    // Initialize artifacts integration if queue is available
    if (this.ingestionQueue) {
      const ingestionService = new JUnitIngestionService();
      this.artifactsIntegration = createGitHubArtifactsIntegration(
        this.authManager,
        this.helpers,
        ingestionService,
        this.prisma
      );
    }
  }

  async process(payload: WorkflowJobWebhookPayload): Promise<void> {
    const { action, workflow_job, repository, installation } = payload;

    logger.info('Processing workflow job webhook', {
      action,
      workflowJobId: workflow_job.id,
      jobName: workflow_job.name,
      repository: repository.full_name,
      status: workflow_job.status,
      conclusion: workflow_job.conclusion,
    });

    // Store workflow job in database
    await this.storeWorkflowJob(payload);

    switch (action) {
      case 'completed':
        await this.handleWorkflowJobCompleted(payload);
        break;
        
      case 'in_progress':
        await this.handleWorkflowJobInProgress(payload);
        break;
        
      case 'queued':
        await this.handleWorkflowJobQueued(payload);
        break;
        
      default:
        logger.warn(`Unhandled workflow job action: ${action}`);
    }
  }

  private async handleWorkflowJobCompleted(payload: WorkflowJobWebhookPayload): Promise<void> {
    const { workflow_job, repository, installation } = payload;
    
    logger.debug('Workflow job completed', {
      workflowJobId: workflow_job.id,
      conclusion: workflow_job.conclusion,
    });

    // For failed test jobs, we might want to trigger immediate ingestion
    // This provides faster feedback than waiting for the entire workflow to complete
    if (workflow_job.conclusion === 'failure' && this.isTestJob(workflow_job)) {
      await this.triggerJobLevelIngestion(payload);
    }
  }

  private async handleWorkflowJobInProgress(payload: WorkflowJobWebhookPayload): Promise<void> {
    const { workflow_job } = payload;
    
    logger.debug('Workflow job in progress', {
      workflowJobId: workflow_job.id,
    });
  }

  private async handleWorkflowJobQueued(payload: WorkflowJobWebhookPayload): Promise<void> {
    const { workflow_job } = payload;
    
    logger.debug('Workflow job queued', {
      workflowJobId: workflow_job.id,
    });
  }

  /**
   * Check if this is a test-related job based on name patterns
   */
  private isTestJob(workflowJob: any): boolean {
    const jobName = workflowJob.name.toLowerCase();
    const testPatterns = ['test', 'unittest', 'integration', 'e2e', 'spec', 'junit'];
    
    return testPatterns.some(pattern => jobName.includes(pattern));
  }

  /**
   * Trigger ingestion for specific job completion (for faster feedback)
   */
  private async triggerJobLevelIngestion(payload: WorkflowJobWebhookPayload): Promise<void> {
    const { workflow_job, repository, installation } = payload;

    // Only trigger if we have the necessary components
    if (!this.ingestionQueue || !this.artifactsIntegration) {
      return;
    }

    try {
      const correlationId = generateCorrelationId();
      
      logger.info('Triggering job-level artifact ingestion', {
        correlationId,
        workflowJobId: workflow_job.id,
        workflowRunId: workflow_job.run_id,
        repository: repository.full_name
      });

      // Check if workflow-level ingestion is already scheduled/running
      const existingWorkflowJob = await this.prisma.ingestionJob.findFirst({
        where: {
          workflowRunId: workflow_job.run_id.toString(),
          repository: {
            githubId: repository.id
          }
        }
      });

      if (existingWorkflowJob && ['queued', 'processing'].includes(existingWorkflowJob.status)) {
        logger.debug('Workflow-level ingestion already active - skipping job-level trigger');
        return;
      }

      // Create a high-priority job for immediate processing
      const jobConfig: IngestionJobConfig = {
        workflowRunId: workflow_job.run_id,
        repository: {
          owner: repository.owner.login,
          repo: repository.name
        },
        installationId: installation.id,
        filter: createTestResultsFilter(),
        correlationId,
        priority: 'critical' // High priority for failed jobs
      };

      // Preview artifacts
      const artifactsPreview = await this.artifactsIntegration.listWorkflowArtifacts(
        repository.owner.login,
        repository.name,
        workflow_job.run_id,
        installation.id,
        createTestResultsFilter()
      );

      if (artifactsPreview.totalCount === 0) {
        logger.debug('No artifacts available for job-level ingestion yet');
        return;
      }

      // Queue with immediate processing for critical failed tests
      await this.queueJobLevelIngestion(jobConfig, artifactsPreview.totalCount, workflow_job.id);

      logger.info('Job-level ingestion queued', {
        correlationId,
        workflowJobId: workflow_job.id,
        artifactCount: artifactsPreview.totalCount
      });

    } catch (error) {
      logger.error('Failed to trigger job-level ingestion', {
        workflowJobId: workflow_job.id,
        error: error instanceof Error ? error.message : String(error)
      });
    }
  }

  /**
   * Queue job-level ingestion with immediate processing
   */
  private async queueJobLevelIngestion(
    config: IngestionJobConfig,
    artifactCount: number,
    workflowJobId: number
  ): Promise<void> {
    const jobId = config.correlationId || generateCorrelationId();

    // Find or create repository record
    const repository = await this.ensureRepository(
      { 
        id: 0,
        owner: { login: config.repository.owner },
        name: config.repository.repo,
        full_name: `${config.repository.owner}/${config.repository.repo}`,
        private: false,
        default_branch: 'main'
      },
      config.installationId.toString()
    );

    // Create database record
    await this.prisma.ingestionJob.create({
      data: {
        id: jobId,
        repositoryId: repository.id,
        workflowRunId: config.workflowRunId.toString(),
        status: 'queued',
        artifactCount,
        correlationId: config.correlationId,
        metadata: {
          filter: config.filter,
          priority: config.priority,
          automatic: true,
          triggeredBy: 'workflow_job.completed',
          workflowJobId: workflowJobId.toString()
        },
        createdAt: new Date()
      }
    });

    // Add to queue with immediate processing for critical jobs
    await this.ingestionQueue!.add(
      'process-artifacts',
      config,
      {
        jobId,
        priority: JobPriorities.CRITICAL,
        delay: 10000, // Minimal delay for immediate processing
        attempts: 3,
        backoff: {
          type: 'exponential',
          delay: 5000
        },
        removeOnComplete: 50,
        removeOnFail: 25
      }
    );
  }

  private async storeWorkflowJob(payload: WorkflowJobWebhookPayload): Promise<void> {
    const { workflow_job, repository, installation } = payload;
    
    try {
      const repoRecord = await this.ensureRepository(repository, installation.id.toString());
      
      await this.prisma.workflowJob.upsert({
        where: { githubId: workflow_job.id },
        update: {
          name: workflow_job.name,
          status: workflow_job.status,
          conclusion: workflow_job.conclusion,
          startedAt: workflow_job.started_at ? new Date(workflow_job.started_at) : null,
          completedAt: workflow_job.completed_at ? new Date(workflow_job.completed_at) : null,
          updatedAt: new Date(),
        },
        create: {
          githubId: workflow_job.id,
          runId: workflow_job.run_id,
          name: workflow_job.name,
          status: workflow_job.status,
          conclusion: workflow_job.conclusion,
          startedAt: workflow_job.started_at ? new Date(workflow_job.started_at) : null,
          completedAt: workflow_job.completed_at ? new Date(workflow_job.completed_at) : null,
          repositoryId: repoRecord.id,
          installationId: installation.id.toString(),
        },
      });
    } catch (error) {
      logger.error('Failed to store workflow job', {
        workflowJobId: workflow_job.id,
        error,
      });
    }
  }

  private async ensureRepository(repository: any, installationId: string): Promise<any> {
    return this.prisma.repository.upsert({
      where: { githubId: repository.id },
      update: {
        name: repository.name,
        fullName: repository.full_name,
        owner: repository.owner.login,
        private: repository.private,
        defaultBranch: repository.default_branch,
        updatedAt: new Date(),
      },
      create: {
        githubId: repository.id,
        nodeId: repository.node_id || `repository-${repository.id}`,
        name: repository.name,
        fullName: repository.full_name,
        owner: repository.owner.login,
        private: repository.private || false,
        defaultBranch: repository.default_branch || 'main',
        installationId,
      },
    });
  }
}

/**
 * Installation webhook handler
 */
export class InstallationHandler extends BaseWebhookProcessor<'installation'> {
  readonly eventType = 'installation' as const;
  
  private readonly prisma: PrismaClient;

  constructor(options: HandlerOptions) {
    super({ logger, metrics: undefined });
    this.prisma = options.prisma;
  }

  async process(payload: InstallationWebhookPayload): Promise<void> {
    const { action, installation } = payload;

    logger.info('Processing installation webhook', {
      action,
      installationId: installation.id,
      account: installation.account.login,
    });

    switch (action) {
      case 'created':
        await this.handleInstallationCreated(payload);
        break;
        
      case 'deleted':
        await this.handleInstallationDeleted(payload);
        break;
        
      case 'suspend':
        await this.handleInstallationSuspended(payload);
        break;
        
      case 'unsuspend':
        await this.handleInstallationUnsuspended(payload);
        break;
        
      case 'new_permissions_accepted':
        await this.handlePermissionsAccepted(payload);
        break;
        
      default:
        logger.warn(`Unhandled installation action: ${action}`);
    }
  }

  private async handleInstallationCreated(payload: InstallationWebhookPayload): Promise<void> {
    const { installation, repositories } = payload;
    
    try {
      // Store installation record
      await this.prisma.installation.create({
        data: {
          id: installation.id.toString(),
          githubInstallationId: installation.id,
          accountLogin: installation.account.login,
          accountId: installation.account.id,
          accountType: installation.account.type,
          repositorySelection: installation.repository_selection,
          permissions: installation.permissions,
          events: installation.events,
          createdAt: new Date(installation.created_at),
          updatedAt: new Date(installation.updated_at),
          suspendedAt: installation.suspended_at ? new Date(installation.suspended_at) : null,
        },
      });

      // Store repository records if provided
      if (repositories) {
        for (const repo of repositories) {
          await this.prisma.repository.upsert({
            where: { githubId: repo.id },
            update: {
              name: repo.name,
              fullName: repo.full_name,
              updatedAt: new Date(),
            },
            create: {
              githubId: repo.id,
              nodeId: repo.node_id || `repository-${repo.id}`,
              name: repo.name,
              fullName: repo.full_name,
              owner: repo.full_name.split('/')[0],
              installationId: installation.id.toString(),
            },
          });
        }
      }

      logger.info('Installation created successfully', {
        installationId: installation.id,
        account: installation.account.login,
        repositoryCount: repositories?.length || 0,
      });

    } catch (error) {
      logger.error('Failed to create installation', {
        installationId: installation.id,
        error: error instanceof Error ? error.message : String(error),
      });
      throw error;
    }
  }

  private async handleInstallationDeleted(payload: InstallationWebhookPayload): Promise<void> {
    const { installation } = payload;
    
    try {
      // Delete installation and cascade to related records
      await this.prisma.installation.delete({
        where: {
          githubInstallationId: installation.id,
        },
      });

      logger.info('Installation deleted successfully', {
        installationId: installation.id,
      });

    } catch (error) {
      logger.error('Failed to delete installation', {
        installationId: installation.id,
        error: error instanceof Error ? error.message : String(error),
      });
      throw error;
    }
  }

  private async handleInstallationSuspended(payload: InstallationWebhookPayload): Promise<void> {
    const { installation } = payload;
    
    try {
      await this.prisma.installation.update({
        where: {
          githubInstallationId: installation.id,
        },
        data: {
          suspendedAt: installation.suspended_at ? new Date(installation.suspended_at) : new Date(),
          updatedAt: new Date(),
        },
      });

      logger.info('Installation suspended', {
        installationId: installation.id,
      });

    } catch (error) {
      logger.error('Failed to suspend installation', {
        installationId: installation.id,
        error,
      });
    }
  }

  private async handleInstallationUnsuspended(payload: InstallationWebhookPayload): Promise<void> {
    const { installation } = payload;
    
    try {
      await this.prisma.installation.update({
        where: {
          githubInstallationId: installation.id,
        },
        data: {
          suspendedAt: null,
          updatedAt: new Date(),
        },
      });

      logger.info('Installation unsuspended', {
        installationId: installation.id,
      });

    } catch (error) {
      logger.error('Failed to unsuspend installation', {
        installationId: installation.id,
        error,
      });
    }
  }

  private async handlePermissionsAccepted(payload: InstallationWebhookPayload): Promise<void> {
    const { installation } = payload;
    
    try {
      await this.prisma.installation.update({
        where: {
          githubInstallationId: installation.id,
        },
        data: {
          permissions: installation.permissions,
          events: installation.events,
          updatedAt: new Date(),
        },
      });

      logger.info('Installation permissions updated', {
        installationId: installation.id,
        permissions: installation.permissions,
      });

    } catch (error) {
      logger.error('Failed to update installation permissions', {
        installationId: installation.id,
        error,
      });
    }
  }
}

/**
 * Create webhook handler instances
 */
export function createWebhookHandlers(options: HandlerOptions): {
  checkRunHandler: CheckRunHandler;
  workflowRunHandler: WorkflowRunHandler;
  workflowJobHandler: WorkflowJobHandler;
  installationHandler: InstallationHandler;
} {
  return {
    checkRunHandler: new CheckRunHandler(options),
    workflowRunHandler: new WorkflowRunHandler(options),
    workflowJobHandler: new WorkflowJobHandler(options),
    installationHandler: new InstallationHandler(options),
  };
}